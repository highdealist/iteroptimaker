{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from newspaper import Article\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from html2text import HTML2Text\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SearchAPI:\n",
    "    def __init__(self, name, base_url, params, user_agent_rotator, results_path):\n",
    "        self.name = name\n",
    "        self.base_url = base_url\n",
    "        self.params = params\n",
    "        self.user_agent_rotator = user_agent_rotator\n",
    "        self.results_path = results_path\n",
    "        self.used = 0\n",
    "        self.last_request_time = 0\n",
    "\n",
    "    async def search(self, session, query: str, num_results: int) -> List[Dict]:\n",
    "        await self.respect_rate_limit()\n",
    "        logger.info(f\"Searching {self.name} for: {query}\")\n",
    "        params = self.params.copy()\n",
    "        params['q'] = query\n",
    "        params['num'] = min(num_results, 10) if self.name == 'Google' else num_results\n",
    "        headers = {'User-Agent': self.user_agent_rotator.random}\n",
    "        try:\n",
    "            async with session.get(self.base_url, params=params, headers=headers, timeout=10) as response:\n",
    "                response.raise_for_status()\n",
    "                self.used += 1\n",
    "                self.last_request_time = time.time()\n",
    "                data = await response.json()\n",
    "                results = []\n",
    "                for item in data.get(self.results_path, []):\n",
    "                    url = item.get('link') or item.get('url')\n",
    "                    title = item.get('title') or \"No title\"\n",
    "                    snippet = item.get('snippet') or \"No snippet\"\n",
    "                    results.append({'title': title, 'url': url, 'snippet': snippet})\n",
    "                return results\n",
    "        except aiohttp.ClientError as e:\n",
    "            logger.error(f\"Error during {self.name} search: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def respect_rate_limit(self):\n",
    "        if time.time() - self.last_request_time < 2:\n",
    "            await asyncio.sleep(2 - (time.time() - self.last_request_time))\n",
    "\n",
    "class WebContentExtractor:\n",
    "    MAX_RETRIES = 2\n",
    "    TIMEOUT = 10\n",
    "    _driver = None\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_driver(cls):\n",
    "        if cls._driver is not None:\n",
    "            return\n",
    "        edge_options = Options()\n",
    "        edge_options.add_argument(\"--headless=new\")\n",
    "        edge_options.add_argument(\"--disable-gpu\")\n",
    "        edge_options.add_argument(\"--no-sandbox\")\n",
    "        user_agent = random.choice(USER_AGENTS)\n",
    "        edge_options.add_argument(f\"user-agent={user_agent}\")\n",
    "        cls._driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=edge_options)\n",
    "        stealth(cls._driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\", webgl_vendor=\"Intel Inc.\", renderer=\"Angle\", fix_hairline=True)\n",
    "\n",
    "    @classmethod\n",
    "    def extract_with_selenium(cls, url: str) -> str:\n",
    "        try:\n",
    "            cls._initialize_driver()\n",
    "            cls._driver.get(url)\n",
    "            WebDriverWait(cls._driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            html_content = cls._driver.page_source\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            main_content = soup.find(['div', 'main', 'article'], class_=CONTENT_CLASS_PATTERN) or soup.body\n",
    "            main_text = main_content.get_text(separator=' ', strip=True) if main_content else ''\n",
    "            return re.sub(r'\\s+', ' ', main_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Selenium extraction failed for {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def extract_content(cls, url: str) -> str:\n",
    "        if not cls.is_valid_url(url):\n",
    "            logger.error(f\"Invalid URL: {url}\")\n",
    "            return \"\"\n",
    "        for extractor in [cls._extract_with_requests, cls._extract_with_newspaper, cls.extract_with_selenium]:\n",
    "            text = extractor(url)\n",
    "            if len(text.strip()) >= 200:\n",
    "                return text\n",
    "        return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _extract_with_requests(cls, url: str) -> str:\n",
    "        for attempt in range(1, cls.MAX_RETRIES + 1):\n",
    "            try:\n",
    "                headers = get_headers()\n",
    "                response = requests.get(url, headers=headers, timeout=cls.TIMEOUT)\n",
    "                response.raise_for_status()\n",
    "                content_type = response.headers.get('Content-Type', '').lower()\n",
    "                if 'text/html' not in content_type:\n",
    "                    logger.warning(f\"Non-HTML content returned for {url}: {content_type}\")\n",
    "                    return \"\"\n",
    "                if response.headers.get('content-encoding') == 'gzip':\n",
    "                    try:\n",
    "                        html_content = gzip.decompress(response.content).decode('utf-8', errors='ignore')\n",
    "                    except (OSError, gzip.BadGzipFile) as e:\n",
    "                        logger.warning(f\"Error decoding gzip content: {e}. Using raw content.\")\n",
    "                        html_content = response.text\n",
    "                else:\n",
    "                    html_content = response.text\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                return cls._extract_content_from_soup(soup)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < cls.MAX_RETRIES:\n",
    "                    logger.warning(f\"Error with requests for {url} (attempt {attempt}): {e}. Retrying...\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    logger.warning(f\"Error with requests for {url} after {cls.MAX_RETRIES} attempts: {e}. Giving up.\")\n",
    "                    return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _extract_with_newspaper(cls, url: str) -> str:\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            return article.text\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Newspaper error for {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_content_from_soup(soup: BeautifulSoup) -> str:\n",
    "        for element in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
    "            element.decompose()\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "        content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main-content|post-content|body|main-body|body-content|main', re.IGNORECASE))\n",
    "        if not content:\n",
    "            content = soup.body\n",
    "        if content:\n",
    "            h = HTML2Text()\n",
    "            h.ignore_links = True\n",
    "            h.ignore_images = True\n",
    "            text = h.handle(str(content))\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = text.strip()\n",
    "            return text\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_url(url: str) -> bool:\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def quit_driver(cls):\n",
    "        if cls._driver is not None:\n",
    "            cls._driver.quit()\n",
    "            cls._driver = None\n",
    "\n",
    "class SearchManager:\n",
    "    def __init__(self, apis: List[SearchAPI], web_search_provider: SearchProvider, max_content_length: int = 10000,\n",
    "                 cache_size: int = 100):\n",
    "        self.apis = apis\n",
    "        self.web_search_provider = web_search_provider\n",
    "        self.content_extractor = WebContentExtractor()\n",
    "        self.max_content_length = max_content_length\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "\n",
    "    async def search(self, query: str, num_results: int = 10):\n",
    "        if query in self.cache:\n",
    "            return self.cache[query]\n",
    "\n",
    "        api_order = [\"Google\", \"Brave\", \"DuckDuckGo\"]\n",
    "        results = []\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for api_name in api_order:\n",
    "                api = next((api for api in self.apis if api.name == api_name), None)\n",
    "                if api and api.is_within_quota():\n",
    "                    try:\n",
    "                        logger.info(f\"Trying {api_name} for query: {query}\")\n",
    "                        search_results = await api.search(session, query, num_results)\n",
    "                        if search_results:\n",
    "                            detailed_results = []\n",
    "                            for result in search_results:\n",
    "                                content = self.content_extractor.extract_content(result['url'])\n",
    "                                result['content'] = content[:self.max_content_length]\n",
    "                                detailed_results.append(result)\n",
    "                            results.extend(detailed_results)\n",
    "                            if len(results) >= num_results:\n",
    "                                break\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error searching {api_name}: {e}\")\n",
    "\n",
    "            if len(results) < num_results:\n",
    "                logger.info(f\"Trying DuckDuckGo for query: {query}\")\n",
    "                duck_results = self.web_search_provider.search(query, num_results)\n",
    "                for result in duck_results:\n",
    "                    content = self.content_extractor.extract_content(result.url)\n",
    "                    results.append({\n",
    "                        'title': result.title,\n",
    "                        'url': result.url,\n",
    "                        'snippet': result.snippet,\n",
    "                        'content': content[:self.max_content_length] if content is not None else \"\"\n",
    "                    })\n",
    "                    if len(results) >= num_results:\n",
    "                        break\n",
    "\n",
    "        self._cache_results(query, results[:num_results])\n",
    "        return results[:num_results]\n",
    "\n",
    "    def _cache_results(self, query: str, results: List[Dict]):\n",
    "        self.cache[query] = results\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "\n",
    "# Example usage\n",
    "# apis = [SearchAPI(...), SearchAPI(...)]\n",
    "# web_search_provider = ...\n",
    "# search_manager = SearchManager(apis, web_search_provider)\n",
    "# results = asyncio.run(search_manager.search(\"example query\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
