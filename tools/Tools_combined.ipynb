{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\__init__.py\n",
    "\n",
    "from .tool_manager import ToolManager\n",
    "from .search_manager import SearchManager, initialize_search_manager\n",
    "from . import read_document\n",
    "from . import fetch_latest_arxiv_papers\n",
    "from . import foia_search\n",
    "from . import get_yt_comments\n",
    "\n",
    "__all__ = [\n",
    "    'ToolManager',\n",
    "    'SearchManager',\n",
    "    'initialize_search_manager',\n",
    "    'read_document',\n",
    "    'fetch_latest_arxiv_papers',\n",
    "    'foia_search',\n",
    "    'get_yt_comments'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\base\\tool.py\n",
    "\n",
    "\"\"\"\n",
    "Base tool interface and implementation.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseTool(ABC):\n",
    "    \"\"\"Abstract base class for all tools.\"\"\"\n",
    "    \n",
    "    def __init__(self, tool_config: Optional[Dict[str, Any]] = None):\n",
    "        self.tool_config = tool_config or {}\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the tool with the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            params: Tool parameters\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing tool execution results\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def tool_name(self) -> str:\n",
    "        \"\"\"Get the name of this tool.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Get the description of this tool.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    def required_params(self) -> list:\n",
    "        \"\"\"Get list of required parameters for this tool.\"\"\"\n",
    "        return []\n",
    "        \n",
    "    def validate_params(self, params: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that all required parameters are present.\n",
    "        \n",
    "        Args:\n",
    "            params: Parameters to validate\n",
    "            \n",
    "        Returns:\n",
    "            True if all required parameters are present, False otherwise\n",
    "        \"\"\"\n",
    "        return all(param in params for param in self.required_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\base_tool.py\n",
    "\n",
    "\"\"\"Base tool class defining the interface for all tools.\"\"\"\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ToolResult:\n",
    "    \"\"\"Container for tool execution results.\"\"\"\n",
    "    success: bool\n",
    "    result: Any\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class BaseTool(ABC):\n",
    "    \"\"\"Abstract base class for all tools.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str, use_case: str = None, operation: str = None):\n",
    "        \"\"\"Initialize a tool.\n",
    "        \n",
    "        Args:\n",
    "            name: Tool name\n",
    "            description: Brief description of the tool's purpose\n",
    "            use_case: Detailed description of when to use this tool\n",
    "            operation: Technical description of how the tool works\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.use_case = use_case\n",
    "        self.operation = operation\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, **kwargs) -> ToolResult:\n",
    "        \"\"\"Execute the tool with the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Tool-specific parameters\n",
    "            \n",
    "        Returns:\n",
    "            ToolResult containing execution status and result\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Get the tool's parameter specifications.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping parameter names to their specifications:\n",
    "            {\n",
    "                \"param_name\": {\n",
    "                    \"type\": str,  # Parameter type\n",
    "                    \"description\": str,  # Parameter description\n",
    "                    \"required\": bool,  # Whether parameter is required\n",
    "                    \"default\": Any  # Default value if any\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\content_extractor.py\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium_stealth import stealth\n",
    "from newspaper import Article\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "from fake_useragent import UserAgent\n",
    "import html2text\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_article_text(url: str) -> Tuple[str, str, Optional[str], str]:\n",
    "    \"\"\"Fetch article text and metadata using newspaper3k.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the article to fetch\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, str, Optional[str], str]: (title, author, publish_date, article_text)\n",
    "    \"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    title = article.title\n",
    "    author = ', '.join(article.authors)\n",
    "    pub_date = article.publish_date\n",
    "    article_text = article.text\n",
    "    \n",
    "    return title, author, pub_date, article_text\n",
    "\n",
    "class WebContentExtractor:\n",
    "    \"\"\"Extracts web content from a given URL with improved error handling and retry logic.\"\"\"\n",
    "    \n",
    "    _driver = None\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, timeout: int = 10):\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self._initialize_driver()\n",
    "    \n",
    "    @classmethod\n",
    "    def get_driver(cls):\n",
    "        \"\"\"Returns the shared WebDriver instance.\"\"\"\n",
    "        if cls._driver is None:\n",
    "            cls._initialize_driver()\n",
    "        return cls._driver\n",
    "    \n",
    "    @classmethod\n",
    "    def _initialize_driver(cls):\n",
    "        \"\"\"Initializes Chrome WebDriver with enhanced anti-detection measures.\"\"\"\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        service = ChromeService(ChromeDriverManager().install())\n",
    "        cls._driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        stealth(cls._driver,\n",
    "                languages=[\"en-US\", \"en\"],\n",
    "                vendor=\"Google Inc.\",\n",
    "                platform=\"Win32\",\n",
    "                webgl_vendor=\"Intel Inc.\",\n",
    "                renderer=\"Intel Iris OpenGL Engine\",\n",
    "                fix_hairline=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def quit_driver(cls):\n",
    "        \"\"\"Quits the WebDriver.\"\"\"\n",
    "        if cls._driver:\n",
    "            cls._driver.quit()\n",
    "            cls._driver = None\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Ensure proper cleanup of WebDriver.\"\"\"\n",
    "        self.quit_driver()\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_url(url: str) -> bool:\n",
    "        \"\"\"Checks if a URL is valid.\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_content_from_soup(soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Helper method to extract and clean content from BeautifulSoup object.\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        for element in soup.find_all(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Remove comments\n",
    "        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_with_requests(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extracts content using requests.\"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': UserAgent().random}\n",
    "            response = requests.get(url, headers=headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return self._extract_content_from_soup(soup)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting content with requests: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_with_newspaper(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extracts content using newspaper3k.\"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            return article.text.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting content with newspaper3k: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_with_selenium(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Extracts content using Selenium with better error handling and wait conditions.\"\"\"\n",
    "        try:\n",
    "            driver = self.get_driver()\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for body to be present\n",
    "            WebDriverWait(driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            return self._extract_content_from_soup(soup)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting content with Selenium: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_content(self, url: str, retry_count: int = 0) -> Optional[str]:\n",
    "        \"\"\"Extract content with automatic retry and fallback mechanisms.\n",
    "        \n",
    "        The extraction methods are tried in the following order:\n",
    "        1. requests with BeautifulSoup (fastest, works for simple pages)\n",
    "        2. newspaper3k (best for article content)\n",
    "        3. Selenium (best for dynamic content, slowest)\n",
    "        \"\"\"\n",
    "        if not self.is_valid_url(url):\n",
    "            logger.error(f\"Invalid URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Try different extraction methods in order\n",
    "        content = None\n",
    "        \n",
    "        # Try requests first (fastest)\n",
    "        content = self._extract_with_requests(url)\n",
    "        if content and len(content.strip()) >= 200:\n",
    "            return content\n",
    "        \n",
    "        # Try newspaper3k second (good for articles)\n",
    "        content = self._extract_with_newspaper(url)\n",
    "        if content and len(content.strip()) >= 200:\n",
    "            return content\n",
    "        \n",
    "        # Finally, try Selenium (best for dynamic content)\n",
    "        content = self.extract_with_selenium(url)\n",
    "        if content and len(content.strip()) >= 200:\n",
    "            return content\n",
    "        \n",
    "        # If all methods fail and we haven't exceeded max retries\n",
    "        if retry_count < self.max_retries:\n",
    "            logger.info(f\"Retrying content extraction for {url} (attempt {retry_count + 1})\")\n",
    "            return self.extract_content(url, retry_count + 1)\n",
    "        \n",
    "        logger.error(f\"All content extraction methods failed for {url}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20875a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\core\\__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\core\\code_analysis_tool.py\n",
    "\n",
    "\"\"\"\n",
    "Code analysis tool implementation.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, List\n",
    "import ast\n",
    "from ..base.tool import BaseTool\n",
    "\n",
    "class CodeAnalysisTool(BaseTool):\n",
    "    \"\"\"Tool for analyzing Python code.\"\"\"\n",
    "    \n",
    "    def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze Python code for various metrics and issues.\n",
    "        \n",
    "        Args:\n",
    "            params: Must contain 'code' key with Python code to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing analysis results\n",
    "        \"\"\"\n",
    "        if not self.validate_params(params):\n",
    "            return {\"error\": \"Missing required parameters\"}\n",
    "            \n",
    "        code = params[\"code\"]\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            return {\n",
    "                \"metrics\": self._compute_metrics(tree),\n",
    "                \"issues\": self._find_issues(tree)\n",
    "            }\n",
    "        except SyntaxError as e:\n",
    "            return {\"error\": f\"Syntax error in code: {str(e)}\"}\n",
    "            \n",
    "    def _compute_metrics(self, tree: ast.AST) -> Dict[str, Any]:\n",
    "        \"\"\"Compute code metrics.\"\"\"\n",
    "        metrics = {\n",
    "            \"num_functions\": len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]),\n",
    "            \"num_classes\": len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]),\n",
    "            \"num_imports\": len([n for n in ast.walk(tree) if isinstance(n, (ast.Import, ast.ImportFrom))]),\n",
    "            \"complexity\": self._compute_complexity(tree)\n",
    "        }\n",
    "        return metrics\n",
    "        \n",
    "    def _compute_complexity(self, tree: ast.AST) -> int:\n",
    "        \"\"\"Compute cyclomatic complexity.\"\"\"\n",
    "        complexity = 0\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.If, ast.While, ast.For, ast.Try, ast.ExceptHandler)):\n",
    "                complexity += 1\n",
    "        return complexity\n",
    "        \n",
    "    def _find_issues(self, tree: ast.AST) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find potential code issues.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for bare excepts\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.ExceptHandler) and node.type is None:\n",
    "                issues.append({\n",
    "                    \"type\": \"bare_except\",\n",
    "                    \"message\": \"Bare except clause found\",\n",
    "                    \"line\": node.lineno\n",
    "                })\n",
    "                \n",
    "        # Check for unused imports\n",
    "        imports = {}\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                for name in node.names:\n",
    "                    imports[name.asname or name.name] = node.lineno\n",
    "                    \n",
    "        # Add more checks as needed\n",
    "        return issues\n",
    "        \n",
    "    @property\n",
    "    def tool_name(self) -> str:\n",
    "        return \"code_analysis\"\n",
    "        \n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Analyze Python code for metrics and potential issues\"\n",
    "        \n",
    "    @property\n",
    "    def required_params(self) -> List[str]:\n",
    "        return [\"code\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e372988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\core\\search_tool.py\n",
    "\n",
    "\"\"\"\n",
    "Search tool implementation.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, List\n",
    "import requests\n",
    "from ..base.tool import BaseTool\n",
    "from ...config import TAVILY_API_KEY\n",
    "\n",
    "class SearchTool(BaseTool):\n",
    "    \"\"\"Tool for performing web searches.\"\"\"\n",
    "    \n",
    "    def __init__(self, tool_config: Dict[str, Any] = None):\n",
    "        super().__init__(tool_config)\n",
    "        self._api_key = TAVILY_API_KEY\n",
    "        \n",
    "    def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute a web search.\n",
    "        \n",
    "        Args:\n",
    "            params: Must contain 'query' key with search query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing search results\n",
    "        \"\"\"\n",
    "        if not self.validate_params(params):\n",
    "            return {\"error\": \"Missing required parameters\"}\n",
    "            \n",
    "        query = params[\"query\"]\n",
    "        max_results = params.get(\"max_results\", 5)\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.tavily.com/search\",\n",
    "                json={\n",
    "                    \"api_key\": self._api_key,\n",
    "                    \"query\": query,\n",
    "                    \"max_results\": max_results\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return {\"results\": response.json()[\"results\"]}\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"error\": f\"Search failed: {str(e)}\"}\n",
    "            \n",
    "    @property\n",
    "    def tool_name(self) -> str:\n",
    "        return \"web_search\"\n",
    "        \n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        return \"Perform web searches to find relevant information\"\n",
    "        \n",
    "    @property\n",
    "    def required_params(self) -> List[str]:\n",
    "        return [\"query\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\extract_with_newspaper.py\n",
    "\n",
    "\n",
    "from newspaper import Article\n",
    "\n",
    "def fetch_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    title = article.title\n",
    "    author = ', '.join(article.authors)\n",
    "    pub_date = article.publish_date\n",
    "    article_text = article.text\n",
    "    \n",
    "    return title, author, pub_date, article_text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'https://www.aha.io/roadmapping/guide/ideation/templates'\n",
    "    title, author, pub_date, article_text = fetch_article_text(url)\n",
    "    \n",
    "    print(f\"Title: {title}\\n\")\n",
    "    print(f\"Author: {author}\\n\")\n",
    "    print(f\"Published Date: {pub_date}\\n\")\n",
    "    print(f\"Article Text: {article_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\fetch_latest_arxiv_papers.py\n",
    "\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "def fetch_latest_arxiv_papers(topic: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch the latest arXiv results for the given topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic : str\n",
    "        The topic to search for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of dictionaries containing the title, authors, summary and\n",
    "        published date of each paper.\n",
    "    \"\"\"\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{topic}&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending'\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "\n",
    "    results = []\n",
    "    for entry in feed.entries:\n",
    "        paper = {\n",
    "            'title': entry.title,\n",
    "            'authors': [author.name for author in entry.authors],\n",
    "            'summary': entry.summary,\n",
    "            'published': entry.published,\n",
    "            'link': entry.link\n",
    "        }\n",
    "        results.append(paper)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    topic = input(\"Enter a topic to search for: \")\n",
    "    results = fetch_latest_arxiv_papers(topic)\n",
    "    for paper in results:\n",
    "        print(f\"Title: {paper['title']}\\n\")\n",
    "        print(f\"Authors: {', '.join(paper['authors'])}\\n\")\n",
    "        print(f\"Published: {paper['published']}\\n\")\n",
    "        print(f\"Summary: {paper['summary']}\\n\")\n",
    "        print(f\"Link: {paper['link']}\\n\")\n",
    "        print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\foia_search.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from typing import List\n",
    "from search_manager import WebContentExtractor\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def foia_search(query: str) -> List[str]:\n",
    "    \"\"\"Searches FOIA.gov for the given query and returns a list of relevant content.\"\"\"\n",
    "    url = f\"https://search.foia.gov/search?utf8=%E2%9C%93&m=true&affiliate=foia.gov&query={query.replace(' ', '+')}\"\n",
    "    web_content_extractor = WebContentExtractor()\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(web_content_extractor.USER_AGENTS),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'DNT': '1',\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=web_content_extractor.TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        result_links = [a['href'] for a in soup.select('.result-title a') if a.has_attr('href')]\n",
    "\n",
    "        content = []\n",
    "        for link in result_links:\n",
    "            try:\n",
    "                if extracted_content := WebContentExtractor.extract_content(link):\n",
    "                    content.append(extracted_content)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting content from {link}: {e}\")\n",
    "\n",
    "        return content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error searching FOIA.gov: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\get_yt_comments.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader\n",
    "\n",
    "# Get the directory of the current file\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "parent_dir = os.path.dirname(current_dir)  # Get the parent directory\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "import models  # Now import models\n",
    "\n",
    "# Get the ModelManager class\n",
    "ModelManager = models.ModelManager\n",
    "\n",
    "# Rest of the code...\n",
    "model_manager = ModelManager()  # Initialize *after* defining ModelManager\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader\n",
    "def fetch_comments(video_url):\n",
    "    downloader = YoutubeCommentDownloader()\n",
    "    return [\n",
    "        comment['text']\n",
    "        for comment in downloader.get_comments_from_url(video_url)\n",
    "    ]\n",
    "\n",
    "def save_comments_to_file(comments, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for comment in comments:\n",
    "            file.write(comment + '\\n')\n",
    "\n",
    "def main():\n",
    "    url = input(\"Enter youtube URL: \")\n",
    "    comments = fetch_comments(url)\n",
    "    save_comments_to_file(comments, 'comments.txt')\n",
    "    model_manager = ModelManager()\n",
    "    prompt = f\"Summarize and extract all insights from the following comment sections for a Youtube Video: \\n {comments}\"\n",
    "    response = model_manager.generate_response(prompt)\n",
    "    for comment in comments:\n",
    "        print(comment)\n",
    "    print(response.text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c390026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\google_search.py\n",
    "\n",
    "import pprint\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "import os\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from ..config import GEMINI_API_KEY, GOOGLE_CUSTOM_SEARCH_API_KEY, GOOGLE_CUSTOM_SEARCH_ENGINE_ID\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "dotenv.load_dotenv()\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY')\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_ID = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_ID')\n",
    "\n",
    "print(GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY)\n",
    "print(GOOGLE_CUSTOM_SEARCH_ENGINE_ID)\n",
    "def search(query):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY)\n",
    "    res = (\n",
    "        service.cse()\n",
    "        .list(\n",
    "            q=query,\n",
    "            cx=GOOGLE_CUSTOM_SEARCH_ENGINE_ID,\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    pprint.pprint(res)\n",
    "    return res\n",
    "\n",
    "search(\"lectures\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Build a service object for interacting with the API. Visit\n",
    "    # the Google APIs Console <http://code.google.com/apis/console>\n",
    "    # to get an API key for your own application.\n",
    "    service = build(\n",
    "        \"customsearch\", \"v1\", developerKey=GOOGLE_CUSTOM_SEARCH_API_KEY\n",
    "    )\n",
    "\n",
    "    res = (\n",
    "        service.cse()\n",
    "        .list(\n",
    "            q=\"lectures\",\n",
    "            cx=\"32e9bbeb5cbee467a:omuauf_lfve\",\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    pprint.pprint(res)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\llm_tools.py\n",
    "\n",
    "\"\"\"LLM-compatible tools using the @tool decorator pattern.\"\"\"\n",
    "from typing import List, Dict, Optional, Any\n",
    "from langchain.tools import tool\n",
    "from .specialized_search import FOIASearchProvider, ArXivSearchProvider\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Standardized search result format for LLM consumption.\"\"\"\n",
    "    title: str\n",
    "    url: str\n",
    "    snippet: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class AsyncRunner:\n",
    "    \"\"\"Helper class to run async functions in sync context.\"\"\"\n",
    "    @staticmethod\n",
    "    def run(coro):\n",
    "        return asyncio.get_event_loop().run_until_complete(coro)\n",
    "\n",
    "@tool\n",
    "def search_foia(query: str, max_results: int = 10) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search FOIA.gov for government records and documents.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find FOIA records\n",
    "        max_results: Maximum number of results to return (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing:\n",
    "        - title: Document title\n",
    "        - url: Document URL\n",
    "        - snippet: Brief description\n",
    "        - content: Full document content if available\n",
    "    \"\"\"\n",
    "    provider = FOIASearchProvider()\n",
    "    results = AsyncRunner.run(provider.search(query, max_results))\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"title\": r.title,\n",
    "            \"url\": r.url,\n",
    "            \"snippet\": r.snippet,\n",
    "            \"content\": r.content\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "\n",
    "@tool\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search arXiv for scientific papers.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find papers\n",
    "        max_results: Maximum number of results to return (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing:\n",
    "        - title: Paper title\n",
    "        - url: arXiv URL\n",
    "        - snippet: Abstract preview\n",
    "        - content: Full paper details including abstract and metadata\n",
    "    \"\"\"\n",
    "    provider = ArXivSearchProvider()\n",
    "    results = AsyncRunner.run(provider.search(query, max_results))\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"title\": r.title,\n",
    "            \"url\": r.url,\n",
    "            \"snippet\": r.snippet,\n",
    "            \"content\": r.content\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "\n",
    "@tool\n",
    "def get_latest_arxiv_papers(\n",
    "    category: Optional[str] = None,\n",
    "    max_results: int = 10,\n",
    "    days: int = 7\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get the latest papers from arXiv, optionally filtered by category.\n",
    "    \n",
    "    Args:\n",
    "        category: arXiv category (e.g., 'cs.AI', 'physics') (optional)\n",
    "        max_results: Maximum number of results to return (default: 10)\n",
    "        days: Number of past days to search (default: 7)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing:\n",
    "        - title: Paper title\n",
    "        - url: arXiv URL\n",
    "        - authors: List of author names\n",
    "        - published_date: Publication date\n",
    "        - updated_date: Last update date\n",
    "        - categories: List of arXiv categories\n",
    "        - abstract: Full paper abstract\n",
    "        - pdf_url: Direct link to PDF\n",
    "    \"\"\"\n",
    "    provider = ArXivSearchProvider()\n",
    "    results = AsyncRunner.run(provider.get_latest_papers(\n",
    "        category=category,\n",
    "        max_results=max_results,\n",
    "        days=days\n",
    "    ))\n",
    "    \n",
    "    formatted_results = []\n",
    "    for r in results:\n",
    "        # Parse the content string into structured data\n",
    "        content_lines = r.content.split('\\n')\n",
    "        metadata = {}\n",
    "        current_field = None\n",
    "        \n",
    "        for line in content_lines:\n",
    "            if line.startswith('Authors:'):\n",
    "                metadata['authors'] = line.replace('Authors:', '').strip().split(', ')\n",
    "            elif line.startswith('Published:'):\n",
    "                metadata['published_date'] = line.replace('Published:', '').strip()\n",
    "            elif line.startswith('Updated:'):\n",
    "                metadata['updated_date'] = line.replace('Updated:', '').strip()\n",
    "            elif line.startswith('Categories:'):\n",
    "                metadata['categories'] = line.replace('Categories:', '').strip().split(', ')\n",
    "            elif line.startswith('Abstract:'):\n",
    "                current_field = 'abstract'\n",
    "                metadata['abstract'] = ''\n",
    "            elif line.startswith('PDF URL:'):\n",
    "                metadata['pdf_url'] = line.replace('PDF URL:', '').strip()\n",
    "            elif current_field == 'abstract' and line.strip():\n",
    "                metadata['abstract'] = metadata.get('abstract', '') + line + '\\n'\n",
    "        \n",
    "        formatted_results.append({\n",
    "            \"title\": r.title,\n",
    "            \"url\": r.url,\n",
    "            **metadata\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def get_llm_tools():\n",
    "    \"\"\"Get all LLM-compatible tools.\"\"\"\n",
    "    return [\n",
    "        search_foia,\n",
    "        search_arxiv,\n",
    "        get_latest_arxiv_papers\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\print_directory_structure.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def get_input_directory():\n",
    "    while True:\n",
    "        if len(sys.argv) > 1:\n",
    "            input_directory = sys.argv[1]\n",
    "        else:\n",
    "            user_input = input('Specify the directory for which you wish to print its structure or press Enter to use the current directory: ')\n",
    "            if user_input == '':\n",
    "                return os.getcwd()\n",
    "            else:\n",
    "                input_directory = user_input\n",
    "        \n",
    "        # Validate the provided path\n",
    "        if not os.path.isdir(input_directory):\n",
    "            print(\"The specified path is not a directory. Please try again.\")\n",
    "            continue\n",
    "        else:\n",
    "            return input_directory\n",
    "\n",
    "def print_directory_structure(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = get_input_directory()\n",
    "    print_directory_structure(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49140de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\read_document.py\n",
    "\n",
    "import os\n",
    "\n",
    "def read_document(file_path: str) -> str:\n",
    "    \"\"\"Reads the content of a document file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the document file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The content of the document.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist.\n",
    "        IOError: If the file cannot be read.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d616e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\research_summary.py\n",
    "\n",
    "from tools.search_manager import SearchManager, initialize_search_manager\n",
    "from tools.tool_manager import ToolManager\n",
    "from models.model_manager import ModelManager\n",
    "from config import GEMINI_API_KEY\n",
    "from models.llm_providers import GeminiProvider\n",
    "from agents.agent import AgentManager, Agent\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ResearchIteration:\n",
    "    query: str\n",
    "    search_results: str\n",
    "    summary: str\n",
    "    extracted_info: Dict[str, str]\n",
    "    timestamp: datetime\n",
    "    confidence_level: float\n",
    "    relevance_score: float\n",
    "    sources: List[Dict[str, str]]\n",
    "\n",
    "class ResearchResponse:\n",
    "    def __init__(self, raw_response: str):\n",
    "        self.raw_response = raw_response\n",
    "        self._parse_response()\n",
    "    \n",
    "    def _parse_response(self):\n",
    "        \"\"\"Parse all sections in one pass for better efficiency.\"\"\"\n",
    "        # Initialize default values\n",
    "        self.search_query = None\n",
    "        self.extracted_info = {}\n",
    "        self.final_report = None\n",
    "        self.confidence_level = 0.0\n",
    "        self.relevance_score = 0.0\n",
    "        \n",
    "        # Define all patterns\n",
    "        patterns = {\n",
    "            'search_query': r\"SEARCH QUERY:\\s*\\{([^}]+)\\}\",\n",
    "            'extracted_info': r\"EXTRACTED INFO:\\s*\\{([^}]+)\\}\",\n",
    "            'final_report': r\"FINAL REPORT:\\s*\\{([^}]+)\\}\",\n",
    "            'confidence': r\"CONFIDENCE:\\s*(\\d*\\.?\\d+)\",\n",
    "            'relevance': r\"RELEVANCE:\\s*(\\d*\\.?\\d+)\"\n",
    "        }\n",
    "        \n",
    "        # Extract all sections in one pass\n",
    "        for section, pattern in patterns.items():\n",
    "            match = re.search(pattern, self.raw_response, re.DOTALL)\n",
    "            if match:\n",
    "                if section == 'extracted_info':\n",
    "                    self.extracted_info = self._parse_info_section(match.group(1))\n",
    "                elif section == 'confidence':\n",
    "                    self.confidence_level = float(match.group(1))\n",
    "                elif section == 'relevance':\n",
    "                    self.relevance_score = float(match.group(1))\n",
    "                else:\n",
    "                    setattr(self, section, match.group(1).strip())\n",
    "\n",
    "    def _parse_info_section(self, info_text: str) -> Dict[str, str]:\n",
    "        \"\"\"Enhanced parsing of the extracted info section.\"\"\"\n",
    "        info_dict = {}\n",
    "        current_key = None\n",
    "        current_value = []\n",
    "        \n",
    "        for line in info_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if ':' in line and not current_key:\n",
    "                key, value = line.split(':', 1)\n",
    "                current_key = key.strip()\n",
    "                current_value = [value.strip()]\n",
    "            elif current_key and line.startswith(' '):\n",
    "                current_value.append(line.strip())\n",
    "            elif current_key:\n",
    "                info_dict[current_key] = '\\n'.join(current_value)\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    current_key = key.strip()\n",
    "                    current_value = [value.strip()]\n",
    "                \n",
    "        if current_key:\n",
    "            info_dict[current_key] = '\\n'.join(current_value)\n",
    "            \n",
    "        return info_dict\n",
    "\n",
    "@dataclass\n",
    "class AnalystFeedback:\n",
    "    relevance_assessment: float\n",
    "    direction_assessment: float\n",
    "    coverage_assessment: float\n",
    "    feedback: str\n",
    "    recommendations: List[str]\n",
    "    timestamp: datetime\n",
    "\n",
    "@dataclass\n",
    "class ResearchLog:\n",
    "    researcher_output: str\n",
    "    analyst_feedback: AnalystFeedback\n",
    "    iteration: int\n",
    "    timestamp: datetime\n",
    "\n",
    "class ResearchAnalyst:\n",
    "    def __init__(self, model_manager: ModelManager):\n",
    "        self.model_manager = model_manager\n",
    "        self.system_prompt = \"\"\"You are an expert research analyst who evaluates the quality, relevance, and direction of ongoing research.\n",
    "        \n",
    "RESPONSE FORMAT INSTRUCTIONS:\n",
    "Always structure your responses using these exact sections:\n",
    "\n",
    "RESEARCH ASSESSMENT: {\n",
    "    relevance_score: [0.0-1.0]  // How relevant is the collected information\n",
    "    direction_score: [0.0-1.0]  // How well aligned with the research goal\n",
    "    coverage_score: [0.0-1.0]   // How comprehensive is the coverage\n",
    "    \n",
    "    strengths: {\n",
    "        - List key strengths of current research direction\n",
    "        - Highlight particularly valuable findings\n",
    "    }\n",
    "    \n",
    "    weaknesses: {\n",
    "        - Identify gaps in coverage\n",
    "        - Point out potential biases or oversights\n",
    "        - Flag any irrelevant tangents\n",
    "    }\n",
    "    \n",
    "    recommendations: {\n",
    "        - Specific suggestions for next steps\n",
    "        - Areas needing more focus\n",
    "        - Topics to avoid or de-prioritize\n",
    "    }\n",
    "}\n",
    "\n",
    "DETAILED FEEDBACK: {\n",
    "    Provide specific, actionable feedback on:\n",
    "    1. Information Quality\n",
    "    2. Research Direction\n",
    "    3. Knowledge Gaps\n",
    "    4. Methodology\n",
    "    5. Source Quality\n",
    "}\n",
    "\n",
    "Your task is to critically analyze research progress and provide guidance to keep the research focused and effective.\"\"\"\n",
    "\n",
    "        self.analyst = self._initialize_analyst()\n",
    "\n",
    "    def _initialize_analyst(self) -> Agent:\n",
    "        \"\"\"Initialize the analyst agent.\"\"\"\n",
    "        try:\n",
    "            agent_manager = AgentManager(tool_manager=None)  # Analyst doesn't need tools\n",
    "            return agent_manager.create_agent(\n",
    "                \"analyst\",\n",
    "                \"research_analyst\",\n",
    "                self.model_manager,\n",
    "                None,\n",
    "                instruction=self.system_prompt,\n",
    "                model_config={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_tokens\": 2048,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize analyst agent: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_research(self, topic: str, research_log: Dict[str, List[ResearchLog]], \n",
    "                        current_knowledge: Dict[str, Dict]) -> AnalystFeedback:\n",
    "        \"\"\"Analyze current research progress and provide feedback.\"\"\"\n",
    "        try:\n",
    "            # Prepare analysis prompt\n",
    "            analysis_prompt = self._prepare_analysis_prompt(topic, research_log, current_knowledge)\n",
    "            \n",
    "            # Get analyst's assessment\n",
    "            response = self.analyst.generate_response(analysis_prompt)\n",
    "            \n",
    "            # Parse analyst's response\n",
    "            feedback = self._parse_analyst_response(response)\n",
    "            \n",
    "            return feedback\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Research analysis failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _prepare_analysis_prompt(self, topic: str, research_log: Dict[str, List[ResearchLog]], \n",
    "                               current_knowledge: Dict[str, Dict]) -> str:\n",
    "        \"\"\"Prepare the prompt for the analyst.\"\"\"\n",
    "        return f\"\"\"Analyze the current research progress on topic: {topic}\n",
    "\n",
    "Research History:\n",
    "{self._format_research_log(research_log)}\n",
    "\n",
    "Current Knowledge State:\n",
    "{json.dumps(current_knowledge, indent=2)}\n",
    "\n",
    "Provide a comprehensive analysis following the required response format.\"\"\"\n",
    "\n",
    "    def _format_research_log(self, research_log: Dict[str, List[ResearchLog]]) -> str:\n",
    "        \"\"\"Format research log for the prompt.\"\"\"\n",
    "        formatted_log = []\n",
    "        for iteration, logs in research_log.items():\n",
    "            formatted_log.append(f\"\\nIteration {iteration}:\")\n",
    "            for log in logs:\n",
    "                formatted_log.append(f\"Researcher Output: {log.researcher_output}\")\n",
    "                formatted_log.append(f\"Previous Analysis: {log.analyst_feedback.feedback}\")\n",
    "        return \"\\n\".join(formatted_log)\n",
    "\n",
    "    def _parse_analyst_response(self, response: str) -> AnalystFeedback:\n",
    "        \"\"\"Parse the analyst's response into structured feedback.\"\"\"\n",
    "        # Extract scores using regex\n",
    "        relevance_score = float(re.search(r\"relevance_score:\\s*(\\d*\\.?\\d+)\", response).group(1))\n",
    "        direction_score = float(re.search(r\"direction_score:\\s*(\\d*\\.?\\d+)\", response).group(1))\n",
    "        coverage_score = float(re.search(r\"coverage_score:\\s*(\\d*\\.?\\d+)\", response).group(1))\n",
    "        \n",
    "        # Extract recommendations\n",
    "        recommendations_match = re.search(r\"recommendations:\\s*\\{([^}]+)\\}\", response, re.DOTALL)\n",
    "        recommendations = [r.strip() for r in recommendations_match.group(1).split('-') if r.strip()]\n",
    "        \n",
    "        # Extract detailed feedback\n",
    "        feedback_match = re.search(r\"DETAILED FEEDBACK:\\s*\\{([^}]+)\\}\", response, re.DOTALL)\n",
    "        feedback = feedback_match.group(1).strip()\n",
    "        \n",
    "        return AnalystFeedback(\n",
    "            relevance_assessment=relevance_score,\n",
    "            direction_assessment=direction_score,\n",
    "            coverage_assessment=coverage_score,\n",
    "            feedback=feedback,\n",
    "            recommendations=recommendations,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, model_manager: ModelManager, tool_manager: ToolManager):\n",
    "        self.model_manager = model_manager\n",
    "        self.tool_manager = tool_manager\n",
    "        self.research_history: List[ResearchIteration] = []\n",
    "        self.knowledge_base: Dict[str, Dict] = {\n",
    "            'facts': {},\n",
    "            'sources': {},\n",
    "            'topics': {},\n",
    "            'uncertainties': {},\n",
    "            'metadata': {\n",
    "                'last_updated': None,\n",
    "                'confidence_history': [],\n",
    "                'relevance_history': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Load agent configuration\n",
    "        with open('agents.json', encoding='utf-8', errors='ignore') as f:\n",
    "            agents_config = json.load(f)\n",
    "            self.researcher_config = agents_config['researcher']\n",
    "        \n",
    "        self.system_prompt = \"\"\"You are an expert research agent that conducts thorough investigations through iterative web searches.\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS:\n",
    "Always structure your responses using these exact sections:\n",
    "\n",
    "SEARCH QUERY: {\n",
    "    Write a specific, focused search query based on current knowledge gaps\n",
    "    Format: \"keyword1 keyword2 -exclude_term site:domain.com\"\n",
    "}\n",
    "\n",
    "EXTRACTED INFO: {\n",
    "    key_fact1: detailed description with source reference\n",
    "    key_fact2: multi-line description\n",
    "               with continuation and source\n",
    "    source1: {url: link, credibility: score, date: timestamp}\n",
    "    uncertainty1: specific areas needing clarification\n",
    "}\n",
    "\n",
    "CONFIDENCE: [0.0-1.0]\n",
    "RELEVANCE: [0.0-1.0]\n",
    "\n",
    "FINAL REPORT: {\n",
    "    Only include this section when confidence >= 0.9 or max iterations reached\n",
    "    Structure the report with:\n",
    "    1. Executive Summary\n",
    "    2. Key Findings\n",
    "    3. Detailed Analysis\n",
    "    4. Sources and Citations\n",
    "    5. Reliability Assessment\n",
    "    6. Further Research Needed\n",
    "}\n",
    "\n",
    "RESEARCH PROCESS:\n",
    "1. Analyze current knowledge gaps\n",
    "2. Formulate precise search queries using search operators\n",
    "3. Extract & organize key information with source tracking\n",
    "4. Assess information reliability and relevance\n",
    "5. Generate comprehensive report when ready\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Information relevance and reliability (scored 0-1)\n",
    "- Source credibility with explicit scoring\n",
    "- Knowledge completeness with gap analysis\n",
    "- Logical connections between facts\n",
    "- Contradictions and uncertainties tracking\n",
    "\n",
    "Your task is to build comprehensive understanding through iterative research while maintaining strict response formatting.\"\"\"\n",
    "        \n",
    "        self.researcher = self._initialize_researcher()\n",
    "        self.research_log = {}\n",
    "        self.current_iteration = 0\n",
    "        self.analyst = ResearchAnalyst(model_manager)\n",
    "\n",
    "    def _initialize_researcher(self) -> Agent:\n",
    "        \"\"\"Initialize the researcher agent with proper configuration.\"\"\"\n",
    "        try:\n",
    "            agent_manager = AgentManager(tool_manager=self.tool_manager)\n",
    "            return agent_manager.create_agent(\n",
    "                \"researcher\",\n",
    "                self.researcher_config['agent_type'],\n",
    "                self.model_manager,\n",
    "                self.tool_manager,\n",
    "                instruction=self.system_prompt,\n",
    "                tools=[\"web_search\"],\n",
    "                model_config={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_tokens\": 2048,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize researcher agent: {e}\")\n",
    "            raise\n",
    "\n",
    "    def update_knowledge_base(self, extracted_info: Dict[str, str], relevance_score: float):\n",
    "        \"\"\"Enhanced knowledge base update with metadata tracking.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Update metadata\n",
    "        self.knowledge_base['metadata']['last_updated'] = current_time\n",
    "        self.knowledge_base['metadata']['relevance_history'].append({\n",
    "            'score': relevance_score,\n",
    "            'timestamp': current_time\n",
    "        })\n",
    "        \n",
    "        # Process extracted information with improved categorization\n",
    "        for key, value in extracted_info.items():\n",
    "            category = self._categorize_info(key)\n",
    "            if category:\n",
    "                if isinstance(value, dict):\n",
    "                    self.knowledge_base[category][key] = {\n",
    "                        'content': value,\n",
    "                        'added': current_time,\n",
    "                        'relevance': relevance_score\n",
    "                    }\n",
    "                else:\n",
    "                    self.knowledge_base[category][key] = {\n",
    "                        'content': value,\n",
    "                        'added': current_time,\n",
    "                        'relevance': relevance_score\n",
    "                    }\n",
    "\n",
    "    def _categorize_info(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Improved information categorization.\"\"\"\n",
    "        prefixes = {\n",
    "            'fact_': 'facts',\n",
    "            'source_': 'sources',\n",
    "            'topic_': 'topics',\n",
    "            'uncertainty_': 'uncertainties'\n",
    "        }\n",
    "        \n",
    "        for prefix, category in prefixes.items():\n",
    "            if key.startswith(prefix):\n",
    "                return category\n",
    "                \n",
    "        # Try to infer category from content\n",
    "        return self._infer_category(key)\n",
    "\n",
    "    def _infer_category(self, key: str) -> str:\n",
    "        \"\"\"Infer the category of information based on content patterns.\"\"\"\n",
    "        key_lower = key.lower()\n",
    "        if any(word in key_lower for word in ['url', 'link', 'source', 'reference']):\n",
    "            return 'sources'\n",
    "        elif any(word in key_lower for word in ['unknown', 'unclear', 'question']):\n",
    "            return 'uncertainties'\n",
    "        elif any(word in key_lower for word in ['topic', 'subject', 'theme']):\n",
    "            return 'topics'\n",
    "        return 'facts'\n",
    "\n",
    "    def conduct_research(self, topic: str, max_iterations: int = 5, results_per_query: int = 5) -> str:\n",
    "        \"\"\"Conduct iterative research on a topic with analyst feedback.\"\"\"\n",
    "        try:\n",
    "            iteration = 0\n",
    "            final_report = None\n",
    "\n",
    "            while iteration < max_iterations:\n",
    "                # Generate research prompt with current knowledge state and analyst feedback\n",
    "                research_prompt = self._prepare_research_prompt(topic, iteration)\n",
    "\n",
    "                # Get structured response from researcher\n",
    "                raw_response = self.researcher.generate_response(research_prompt)\n",
    "                response = ResearchResponse(raw_response)\n",
    "\n",
    "                # Get analyst feedback\n",
    "                analyst_feedback = self.analyst.analyze_research(\n",
    "                    topic,\n",
    "                    self.research_log,\n",
    "                    self.knowledge_base\n",
    "                )\n",
    "\n",
    "                # Store research log\n",
    "                if iteration not in self.research_log:\n",
    "                    self.research_log[iteration] = []\n",
    "                \n",
    "                self.research_log[iteration].append(ResearchLog(\n",
    "                    researcher_output=raw_response,\n",
    "                    analyst_feedback=analyst_feedback,\n",
    "                    iteration=iteration,\n",
    "                    timestamp=datetime.now()\n",
    "                ))\n",
    "\n",
    "                if response.search_query:\n",
    "                    # Perform web search\n",
    "                    try:\n",
    "                        search_results = self.tool_manager.web_search(\n",
    "                            response.search_query,\n",
    "                            results_per_query\n",
    "                        )\n",
    "                        \n",
    "                        # Update knowledge base with new information\n",
    "                        self.update_knowledge_base(response.extracted_info, response.relevance_score)\n",
    "                        \n",
    "                        # Store research iteration\n",
    "                        self.research_history.append(ResearchIteration(\n",
    "                            query=response.search_query,\n",
    "                            search_results=search_results,\n",
    "                            summary=raw_response,\n",
    "                            extracted_info=response.extracted_info,\n",
    "                            timestamp=datetime.now(),\n",
    "                            confidence_level=response.confidence_level,\n",
    "                            relevance_score=response.relevance_score,\n",
    "                            sources=response.sources\n",
    "                        ))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Search failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # Check if we should continue based on analyst feedback\n",
    "                if analyst_feedback.coverage_assessment >= 0.9 and analyst_feedback.relevance_assessment >= 0.9:\n",
    "                    final_report = self.generate_final_report(topic)\n",
    "                    break\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "            # Generate final report if not already done\n",
    "            if not final_report:\n",
    "                final_report = self.generate_final_report(topic)\n",
    "\n",
    "            return final_report\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Research process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _prepare_research_prompt(self, topic: str, iteration: int) -> str:\n",
    "        \"\"\"Prepare research prompt with analyst feedback.\"\"\"\n",
    "        prompt = f\"\"\"Research Topic: {topic}\n",
    "\n",
    "Current Knowledge Base:\n",
    "{json.dumps(self.knowledge_base, indent=2)}\n",
    "\n",
    "Research History and Analysis:\n",
    "{self._format_research_history(iteration)}\n",
    "\n",
    "Analyze the current state of research and respond in the required format with either:\n",
    "1. A new search query to fill knowledge gaps, or\n",
    "2. A final report if confidence level is sufficient (>=0.9)\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _format_research_history(self, current_iteration: int) -> str:\n",
    "        \"\"\"Format research history with analyst feedback.\"\"\"\n",
    "        history = []\n",
    "        for iteration in range(current_iteration):\n",
    "            if iteration in self.research_log:\n",
    "                for log in self.research_log[iteration]:\n",
    "                    history.append(f\"\\nIteration {iteration}:\")\n",
    "                    history.append(f\"Research Output: {log.researcher_output}\")\n",
    "                    history.append(f\"Analyst Feedback: {log.analyst_feedback.feedback}\")\n",
    "                    history.append(\"Recommendations:\")\n",
    "                    for rec in log.analyst_feedback.recommendations:\n",
    "                        history.append(f\"- {rec}\")\n",
    "        return \"\\n\".join(history)\n",
    "\n",
    "    def generate_final_report(self, topic: str) -> str:\n",
    "        \"\"\"Generate a final report based on accumulated knowledge.\"\"\"\n",
    "        report_prompt = f\"\"\"Generate a comprehensive final report on '{topic}' using the accumulated knowledge:\n",
    "\n",
    "Knowledge Base:\n",
    "{json.dumps(self.knowledge_base, indent=2)}\n",
    "\n",
    "Research History:\n",
    "{json.dumps([{\n",
    "    'query': iter.query,\n",
    "    'summary': iter.summary,\n",
    "    'confidence': iter.confidence_level\n",
    "} for iter in self.research_history], indent=2)}\n",
    "\n",
    "Provide a complete report in the FINAL REPORT format.\"\"\"\n",
    "\n",
    "        response = self.researcher.generate_response(report_prompt)\n",
    "        parsed_response = ResearchResponse(response)\n",
    "        return parsed_response.final_report or \"Failed to generate final report\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Initialize required components\n",
    "        search_manager = initialize_search_manager()\n",
    "        tool_manager = tools.ToolManager(search_manager)\n",
    "        model_manager = ModelManager()\n",
    "        \n",
    "        # Create research agent\n",
    "        researcher = ResearchAgent(model_manager, tool_manager)\n",
    "        \n",
    "        # Get user input\n",
    "        topic = input(\"Enter research topic: \")\n",
    "        max_iterations = int(input(\"Enter maximum research iterations (1-5): \"))\n",
    "        results_per_query = int(input(\"Enter results per search (1-10): \"))\n",
    "        \n",
    "        # Validate inputs\n",
    "        max_iterations = min(max(1, max_iterations), 5)\n",
    "        results_per_query = min(max(1, results_per_query), 10)\n",
    "        \n",
    "        # Conduct research and generate report\n",
    "        report = researcher.conduct_research(\n",
    "            topic, \n",
    "            max_iterations=max_iterations,\n",
    "            results_per_query=results_per_query\n",
    "        )\n",
    "        \n",
    "        # Print report\n",
    "        print(\"\\nFinal Research Report:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Research process failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\scrape_current_window_url.py\n",
    "\n",
    "import pygetwindow as gw\n",
    "import pyperclip\n",
    "import keyboard\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory of this file\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Get the parent directory (project root)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Add the project root to the Python path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from search_manager import WebContentExtractor, SearchManager, SearchProvider, SearchAPI, DuckDuckGoSearchProvider\n",
    "import datetime\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def is_browser_window(window_title):\n",
    "    \"\"\"Check if the current window is a browser window.\"\"\"\n",
    "    browsers = [\"Microsoft Edge\", \"Google Chrome\", \"Firefox\", \"Safari\", \"Opera\"]\n",
    "    return any(browser in window_title for browser in browsers)\n",
    "\n",
    "def setup_hotkey():\n",
    "    \"\"\"Set up a hotkey (Ctrl+Alt+C) to capture and scrape the current browser URL.\"\"\"\n",
    "    def on_hotkey():\n",
    "        url = capture_url()\n",
    "        if url:\n",
    "            content_extractor = WebContentExtractor()\n",
    "            content = content_extractor.extract_content(url)\n",
    "            save_to_file(url, content)\n",
    "            print(f\"Captured and saved content from: {url}\")\n",
    "        else:\n",
    "            print(\"No valid URL found in current window\")\n",
    "\n",
    "    keyboard.add_hotkey('ctrl+alt+c', on_hotkey)\n",
    "    print(\"Hotkey (Ctrl+Alt+C) registered for URL capture\")\n",
    "\n",
    "\n",
    "def get_edge_url_with_psutil():\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        if proc.info['name'] == 'msedge.exe':\n",
    "            cmdline = proc.info['cmdline']\n",
    "            for arg in cmdline:\n",
    "                if arg.startswith('https://'):\n",
    "                    return arg\n",
    "\n",
    "def capture_url():\n",
    "    # Get the active Edge window\n",
    "    try:\n",
    "        edge_window = gw.getActiveWindow()\n",
    "\n",
    "        if \"Microsoft Edge\" not in edge_window.title:\n",
    "            return None\n",
    "        # Get the URL from the clipboard\n",
    "        url = pyperclip.paste()\n",
    "\n",
    "        if not url.startswith(\"http\"):\n",
    "            # If the clipboard doesn't contain a URL, try to get the URL from the Edge window title\n",
    "            url = edge_window.title.split(\" - \")[0]\n",
    "        if not url.startswith(\"https://\"):\n",
    "            url = get_edge_url_with_psutil()\n",
    "            \n",
    "        return url\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def save_to_file(url, content):\n",
    "    \"\"\"Save the URL and content to a new text file with a timestamp on the desktop.\"\"\"\n",
    "    if url:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        desktop = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')\n",
    "        with open(os.path.join(desktop, f\"Scraped_URL_{timestamp}.txt\"), \"w\") as f:\n",
    "            f.write(f\"{url}\\n{content}\")\n",
    "\n",
    "\n",
    "url = capture_url()\n",
    "if url:\n",
    "    content_extractor = WebContentExtractor()\n",
    "    content = content_extractor.extract_content(url)\n",
    "    save_to_file(url, content)\n",
    "    print(f\"Captured URL: {url}\")\n",
    "    print(f\"Captured content: {content}\")\n",
    "input(\"Press Enter to exit...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\scrape_url_in_clipboard.py\n",
    "\n",
    "import pyperclip\n",
    "from ..agent_tools.search_manager import search_manager, WebContentExtractor    \n",
    "\n",
    "def main():\n",
    "    url = pyperclip.paste()\n",
    "    content = WebContentExtractor.extract_content(url)\n",
    "    # Print the URL to verify\n",
    "    print(f\"The content from: {url}\\n{content}\")\n",
    "def scrape_url_from_clipboard():\n",
    "    \"\"\"\n",
    "    Scrapes content from URL in clipboard and returns the extracted content.\n",
    "    Returns:\n",
    "        tuple: (url, content) where url is the URL that was scraped and content is the extracted text\n",
    "    \"\"\"\n",
    "    url = pyperclip.paste()\n",
    "    content = WebContentExtractor.extract_content(url)\n",
    "    return url, content\n",
    "\n",
    "# Update main to use the new function\n",
    "def main():\n",
    "    url, content = scrape_url_from_clipboard()\n",
    "    # Print the URL to verify  \n",
    "    print(f\"The content from: {url}\\n{content}\")\n",
    "    input(\"Press Enter to continue...\")\n",
    "    if input(\"Do you want to save the content to a file? (y/n): \") == \"y\":\n",
    "        file_name = input(\"Enter the name of the file: \")\n",
    "        with open(file_name, \"w\") as file:\n",
    "            file.write(content)\n",
    "        print(f\"Content saved to {file_name}\")  \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "            \n",
    "    #Usage:\n",
    "    #1. Copy the URL to the clipboard\n",
    "    #2. Run the script\n",
    "    #3. The script will print the content of the URL to the console\n",
    "    \n",
    "    #Call from another script:\n",
    "    #url, content = scrape_url_from_clipboard()\n",
    "    #print(f\"The content from: {url}\\n{content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388414fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\scrape_w_playwright_v2.py\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Dict, Optional, NamedTuple\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from playwright.async_api import async_playwright, Error as PlaywrightError\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "import yaml\n",
    "from aiohttp import ClientSession, ClientError\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from functools import wraps\n",
    "import time\n",
    "import rate_limiter\n",
    "\n",
    "retries=3\n",
    "timeout=30\n",
    "results_per_page=5  # Add this if not present\n",
    "pages_to_scrape=5\n",
    "rate_limit=1.0\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ProxyConfig:\n",
    "    server: str\n",
    "    port: int\n",
    "    type: str\n",
    "    failures: int = 0\n",
    "    last_used: float = 0\n",
    "\n",
    "@dataclass\n",
    "class ScraperConfig:\n",
    "    query: str\n",
    "    retries: int\n",
    "    timeout: int\n",
    "    proxy_configs: List[ProxyConfig]\n",
    "    pages_to_scrape: int\n",
    "    results_per_page: int\n",
    "    rate_limit: float\n",
    "\n",
    "class SearchResult(NamedTuple):\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "def singleton(cls):\n",
    "    instances = {}\n",
    "    @wraps(cls)\n",
    "    def get_instance(*args, **kwargs):\n",
    "        if cls not in instances:\n",
    "            instances[cls] = cls(*args, **kwargs)\n",
    "        return instances[cls]\n",
    "    return get_instance\n",
    "\n",
    "def load_config(file_path: str) -> ScraperConfig:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        proxies = []\n",
    "        for p in config['proxy_configs']:\n",
    "            parsed_proxy = urlparse(p)\n",
    "            if parsed_proxy.scheme and parsed_proxy.hostname and parsed_proxy.port:\n",
    "                proxies.append(ProxyConfig(server=parsed_proxy.hostname, port=parsed_proxy.port, type=parsed_proxy.scheme))\n",
    "            else:\n",
    "                logger.warning(f\"Invalid proxy format: {p}. Skipping.\")\n",
    "        return ScraperConfig(\n",
    "            query=config['query'],\n",
    "            retries=config['retries'],\n",
    "            timeout=config['timeout'],\n",
    "            proxy_configs=proxies,\n",
    "            pages_to_scrape=config.get('pages_to_scrape', 5),\n",
    "            results_per_page=config.get('results_per_page', 5),\n",
    "            rate_limit=config.get('rate_limit', 1.0)\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Config file not found: {file_path}\")\n",
    "        raise\n",
    "    except yaml.YAMLError:\n",
    "        logger.error(f\"Invalid YAML in config file: {file_path}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Missing required key in config: {str(e)}\")\n",
    "        raise\n",
    "     \n",
    "@singleton\n",
    "class UserAgentRotator:\n",
    "    def __init__(self):\n",
    "        self.ua = UserAgent()\n",
    "        self.user_agents = [self.ua.chrome, self.ua.firefox, self.ua.safari, self.ua.edge]\n",
    "\n",
    "    def get_random_user_agent(self):\n",
    "        return random.choice(self.user_agents)\n",
    "\n",
    "ua_rotator = UserAgentRotator()\n",
    "\n",
    "class ProxyManager:\n",
    "    def __init__(self, proxies: List[ProxyConfig]):\n",
    "        self.proxies = proxies\n",
    "        self.current_index = 0\n",
    "\n",
    "    def get_next_proxy(self) -> ProxyConfig:\n",
    "        proxy = self.proxies[self.current_index]\n",
    "        self.current_index = (self.current_index + 1) % len(self.proxies)\n",
    "        return proxy\n",
    "\n",
    "    def mark_proxy_failure(self, proxy: ProxyConfig):\n",
    "        proxy.failures += 1\n",
    "        if proxy.failures > 3:\n",
    "            self.proxies.remove(proxy)\n",
    "            logger.warning(f\"Removed failing proxy: {proxy.server}:{proxy.port}\")\n",
    "\n",
    "    def mark_proxy_success(self, proxy: ProxyConfig):\n",
    "        proxy.failures = 0\n",
    "        proxy.last_used = time.time()\n",
    "\n",
    "def fetch_with_proxy(ua_rotator: UserAgentRotator, proxy_manager: ProxyManager, rate_limiter: AsyncLimiter, **kwargs) -> Optional[str]:\n",
    "    url = kwargs['url']\n",
    "    timeout = kwargs['timeout']\n",
    "\n",
    "    for _ in range(3):  # Try up to 3 different proxies\n",
    "        proxy = proxy_manager.get_next_proxy()\n",
    "        try:\n",
    "            with rate_limiter:\n",
    "                with async_playwright() as p:\n",
    "                    browser = p.chromium.launch(\n",
    "                        proxy={\"server\": f\"{proxy.type}://{proxy.server}:{proxy.port}\"},\n",
    "                        headless=random.choice([True, False])\n",
    "                    )\n",
    "                    with browser.new_context(\n",
    "                        user_agent=ua_rotator.get_random_user_agent(),\n",
    "                        viewport={'width': random.randint(1024, 1920), 'height': random.randint(768, 1080)}\n",
    "                    ) as context:\n",
    "                        with context.new_page() as page:\n",
    "                            simulate_human_behavior(page)\n",
    "                            page.goto(url, timeout=timeout, wait_until='networkidle')\n",
    "                            simulate_browsing(page)\n",
    "                            content = page.content()\n",
    "                            proxy_manager.mark_proxy_success(proxy)\n",
    "                            return content\n",
    "        except PlaywrightError as e:\n",
    "            logger.error(f\"Playwright error with proxy {proxy.server}: {str(e)}\")\n",
    "        except TimeoutError as e:\n",
    "            logger.error(f\"Timeout error with proxy {proxy.server}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error with proxy {proxy.server}: {str(e)}\")\n",
    "        \n",
    "        proxy_manager.mark_proxy_failure(proxy)\n",
    "\n",
    "    logger.error(f\"Failed to fetch {url} after trying multiple proxies\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def simulate_human_behavior(page) -> None:\n",
    "    try:\n",
    "        page.mouse.move(random.randint(0, 1920), random.randint(0, 1080))\n",
    "        asyncio.sleep(random.uniform(2, 5))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error simulating human behavior: {str(e)}\")\n",
    "\n",
    "def simulate_browsing(page) -> None:\n",
    "    try:\n",
    "        asyncio.sleep(random.uniform(3, 7))\n",
    "        for _ in range(random.randint(2, 5)):\n",
    "            page.evaluate(\"window.scrollBy(0, Math.floor(Math.random() * window.innerHeight))\")\n",
    "            asyncio.sleep(random.uniform(1, 3))\n",
    "\n",
    "            if random.random() < 0.3:\n",
    "                page.mouse.click(random.randint(0, 1920), random.randint(0, 1080))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error simulating browsing: {str(e)}\")\n",
    "\n",
    "def parse_search_results(html_content: str) -> List[SearchResult]:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        results = []\n",
    "        for item in soup.select('div.g'):\n",
    "            title_elem = item.select_one('h3')\n",
    "            url_elem = item.select_one('div.yuRUbf > a')\n",
    "            if title_elem and url_elem:\n",
    "                title = title_elem.get_text()\n",
    "                url = url_elem.get('href')\n",
    "                if title and url:\n",
    "                    results.append(SearchResult(title=title, url=url))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing search results: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def scrape_url(url: str, context) -> str:\n",
    "    try:\n",
    "        page = context.new_page()\n",
    "        page.goto(url, wait_until='networkidle')\n",
    "        simulate_browsing(page)\n",
    "        content = page.content()\n",
    "        page.close()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping result page {url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def scrape_urls(content: str, context, results_per_page: int) -> List[Dict[str, str]]:\n",
    "    results = parse_search_results(content)\n",
    "    scraped_results = []\n",
    "    # Use the results_per_page parameter instead of hardcoded 5\n",
    "    for result in results[:10]:\n",
    "        try:\n",
    "            page_content = scrape_url(result.url, context)\n",
    "            scraped_text = extract_text_from_html(page_content)\n",
    "            scraped_results.append({\n",
    "                'title': result.title,\n",
    "                'url': result.url,\n",
    "                'content': scraped_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting and scraping result {result.url}: {str(e)}\")\n",
    "    return scraped_results\n",
    "\n",
    "\n",
    "def extract_text_from_html(html_content: str) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        return soup.get_text(separator=' ', strip=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from HTML: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    retry_error_callback=lambda _: None\n",
    ")\n",
    "def fetch_with_retry(url: str, proxy_config: ProxyConfig, timeout: float) -> Optional[str]:\n",
    "    proxy_url = f\"{proxy_config.type}://{proxy_config.server}:{proxy_config.port}\"\n",
    "    headers = {\n",
    "        'User-Agent': ua_rotator.get_random_user_agent(),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with ClientSession() as session:\n",
    "            with session.get(url, proxy=proxy_url, timeout=timeout, headers=headers, allow_redirects=True) as response:\n",
    "                logger.info(f\"Response status: {response.status}\")\n",
    "                logger.info(f\"Response headers: {response.headers}\")\n",
    "                asyncio.sleep(random.uniform(1, 3))\n",
    "                response.raise_for_status()\n",
    "                return response.text()\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Client error during fetch: {str(e)}\")\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.error(f\"Timeout error during fetch for URL: {url}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during fetch: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_pages(config: ScraperConfig, context, proxy_manager: ProxyManager, rate_limiter: AsyncLimiter):\n",
    "    all_results = []\n",
    "    for page_num in range(config.pages_to_scrape):\n",
    "        url = f\"https://www.google.com/search?q={config.query}&start={page_num * 10}\"\n",
    "        content = fetch_with_proxy(ua_rotator, proxy_manager, rate_limiter, url=url, timeout=config.timeout)\n",
    "        if content:\n",
    "            results = scrape_urls(content, context, config.results_per_page)\n",
    "            all_results.extend(results)\n",
    "    return all_results\n",
    "\n",
    "def save_results_to_csv(results: List[Dict[str, str]], filename: str):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['title', 'url', 'content'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    logger.info(f\"Scraping completed. {len(results)} results saved to {filename}\")\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    try:\n",
    "        config = load_config('scraper_config.yaml')\n",
    "        proxy_manager = ProxyManager(config.proxy_configs)\n",
    "        rate_limiter = Rate_limiter(1, config.rate_limit)\n",
    "        \n",
    "        with async_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            with browser.new_context(user_agent=ua_rotator.get_random_user_agent()) as context:\n",
    "                all_results = scrape_pages(config, context, proxy_manager, rate_limiter)\n",
    "        \n",
    "        save_results_to_csv(all_results, 'search_results_with_content.csv')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\search_api.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from fake_useragent import UserAgent\n",
    "import html2text\n",
    "from duckduckgo_search import DDGS\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium_stealth import stealth\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import gzip\n",
    "#$end\n",
    "from newspaper import Article\n",
    "from functools import lru_cache\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    title = article.title\n",
    "    author = ', '.join(article.authors)\n",
    "    pub_date = article.publish_date\n",
    "    article_text = article.text\n",
    "    \n",
    "    return title, author, pub_date, article_text\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Google API keys\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_ID = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_ID')\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY= os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY')\n",
    "BRAVE_SEARCH_API_KEY = os.getenv('BRAVE_SEARCH_API_KEY')  # Brave Search API key (if available)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)  # Get a logger instance\n",
    "\n",
    "# --- [Improved] More descriptive error handling ---\n",
    "def initialize_apis() -> List['SearchAPI']:\n",
    "    \"\"\"Initializes the APIs.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchAPI]: A list of initialized SearchAPI objects.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If a required environment variable for an API is not set. \n",
    "    \"\"\"\n",
    "    if GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY is None or GOOGLE_CUSTOM_SEARCH_ENGINE_ID is None:\n",
    "        raise ValueError(\"GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID must be set in .env.\")\n",
    "    apis = [\n",
    "        SearchAPI(\n",
    "            \"Google\",\n",
    "            GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY,\n",
    "            \"https://www.googleapis.com/customsearch/v1\",\n",
    "            {\"cx\": GOOGLE_CUSTOM_SEARCH_ENGINE_ID},\n",
    "            100,\n",
    "            'items',\n",
    "            1,\n",
    "        )\n",
    "    ]\n",
    "    if BRAVE_SEARCH_API_KEY:\n",
    "        apis.append(SearchAPI(\"Brave\", BRAVE_SEARCH_API_KEY, \"https://api.search.brave.com/res/v1/web/search\",\n",
    "                            {}, 2000, 'results', 1))\n",
    "\n",
    "    apis.append(SearchAPI(\"DuckDuckGo\", \"\", \"https://api.duckduckgo.com/\",\n",
    "                        {\"format\": \"json\"}, float('inf'), 'RelatedTopics', 0)) \n",
    "\n",
    "    return apis\n",
    "# --- (end) ---\n",
    "\n",
    "\n",
    "def configure_search_settings() -> Dict[str, Any]:\n",
    "    \"\"\"Prompts the user to enable/disable search functionality.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing search settings, \n",
    "                        including 'search_enabled' (bool) and, if enabled, \n",
    "                        'all_search_result_data', 'search_session_counter', \n",
    "                        'search_session_id', and 'apis'. \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Do you want to enable search functionality? (Y/N): \").strip().lower()\n",
    "            if user_input == 'y':\n",
    "                return {\n",
    "                    'search_enabled': True,\n",
    "                    'all_search_result_data': {},\n",
    "                    'search_session_counter': 0,\n",
    "                    'search_session_id': 0,\n",
    "                    'apis': initialize_apis(),\n",
    "                }\n",
    "            elif user_input == 'n':\n",
    "                return {'search_enabled': False}\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter Y or N.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}. Please try again.\")\n",
    "\n",
    "\n",
    "class SearchProvider(ABC):\n",
    "    \"\"\"Abstract base class for search providers.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def search(self, query: str, num_results: int) -> List['SearchResult']:\n",
    "        \"\"\"Perform a search and return a list of SearchResult objects.\"\"\"\n",
    "        pass \n",
    "\n",
    "\n",
    "class SearchResult:\n",
    "    \"\"\"Represents a single search result.\"\"\"\n",
    "\n",
    "    def __init__(self, title: str, url: str, snippet: str, content: str = \"\"):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.snippet = snippet\n",
    "        self.content = content\n",
    "\n",
    "\n",
    "class SearchAPI(SearchProvider):\n",
    "    \"\"\"Represents a search API with rate limiting and quota management.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, api_key: str, base_url: str, params: dict, quota: int, results_path: str,\n",
    "                 rate_limit: int):\n",
    "        self.name = name\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.params = params.copy()  # Create a copy to avoid modifying the original\n",
    "        if api_key:\n",
    "            self.params['key'] = api_key\n",
    "        self.quota = quota\n",
    "        self.used = 0\n",
    "        self.results_path = results_path\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.user_agent_rotator = UserAgent()\n",
    "\n",
    "    def is_within_quota(self) -> bool:\n",
    "        \"\"\"Checks if the API is within its usage quota.\"\"\"\n",
    "        return self.used < self.quota\n",
    "\n",
    "    def respect_rate_limit(self):\n",
    "        \"\"\"Pauses execution to respect the API's rate limit.\"\"\"\n",
    "        time_since_last_request = time.time() - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last_request)\n",
    "\n",
    "    def search(self, query: str, num_results: int) -> List[SearchResult]:\n",
    "        \"\"\"Performs a search using the API.\"\"\"\n",
    "        self.respect_rate_limit()\n",
    "        logger.info(f\"Searching {self.name} for: {query}\")\n",
    "        params = self.params.copy()\n",
    "        params['q'] = query\n",
    "\n",
    "        # Google Custom Search has a max of 10 results per request\n",
    "        params['num'] = min(num_results, 10) if self.name == 'Google' else num_results\n",
    "        headers = {'User-Agent': self.user_agent_rotator.random}\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            self.used += 1\n",
    "            self.last_request_time = time.time()\n",
    "            data = response.json()\n",
    "\n",
    "            results = []\n",
    "            for item in data.get(self.results_path, []):\n",
    "                url = item.get('link') or item.get('url')\n",
    "                title = item.get('title') or \"No title\"\n",
    "                snippet = item.get('snippet') or \"No snippet\"\n",
    "                results.append(SearchResult(title, url, snippet))\n",
    "            return results\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error during {self.name} search: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class DuckDuckGoSearchProvider(SearchProvider):\n",
    "    \"\"\"Provides search functionality using DuckDuckGo.\"\"\"\n",
    "\n",
    "    def search(self, query: str, max_results: int) -> List[SearchResult]:\n",
    "        \"\"\"Searches DuckDuckGo and returns a list of SearchResult objects.\"\"\"\n",
    "        try:\n",
    "            sanitized_query = self._sanitize_query(query)\n",
    "            with DDGS() as ddgs:\n",
    "                results = list(ddgs.text(sanitized_query, region='wt-wt', safesearch='off', timelimit='y'))[\n",
    "                          :max_results]\n",
    "            return [SearchResult(r['title'], r['href'], r['body']) for r in results]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching DuckDuckGo: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_query(self, query: str) -> str:\n",
    "        \"\"\"Sanitizes the search query for DuckDuckGo.\"\"\"\n",
    "        query = re.sub(r'[^\\w\\s]', '', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "        return query[:5000]\n",
    "\n",
    "\n",
    "class WebContentExtractor:\n",
    "    \"\"\"Extracts web content from a given URL.\"\"\"\n",
    "    MAX_RETRIES = 2\n",
    "    TIMEOUT = 5\n",
    "    USER_AGENTS = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',\n",
    "        'Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/91.0.4472.80 Mobile/15E148 Safari/604.1',\n",
    "        'Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36 OPR/78.0.4093.147',\n",
    "    ]\n",
    "        # Class variable to hold the WebDriver instance\n",
    "    _driver = None \n",
    "\n",
    "    @classmethod\n",
    "    def get_driver(cls):\n",
    "        \"\"\"Returns the shared WebDriver instance.\"\"\"\n",
    "        if cls._driver is None:\n",
    "            cls._initialize_driver()\n",
    "        return cls._driver\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_driver(cls):\n",
    "        \"\"\"Initializes the Selenium WebDriver with anti-detection measures.\"\"\"\n",
    "        edge_options = Options()\n",
    "        edge_options.add_argument(\"--headless=new\")\n",
    "        edge_options.add_argument(\"--disable-gpu\")\n",
    "        edge_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "        user_agent = random.choice(cls.USER_AGENTS)\n",
    "        edge_options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "        cls._driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=edge_options)\n",
    "\n",
    "        stealth(cls._driver,\n",
    "                languages=[\"en-US\", \"en\"],\n",
    "                vendor=\"Google Inc.\",\n",
    "                platform=\"Win32\",\n",
    "                webgl_vendor=\"Intel Inc.\",\n",
    "                renderer=\"Angle\",\n",
    "                fix_hairline=True, \n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def quit_driver(cls):\n",
    "        \"\"\"Quits the WebDriver.\"\"\"\n",
    "        if cls._driver is not None:\n",
    "            cls._driver.quit()\n",
    "            cls._driver = None\n",
    "            \n",
    "    @classmethod\n",
    "    def extract_with_selenium(cls, url: str) -> str:\n",
    "        \"\"\"Extracts content using Selenium as a fallback.\"\"\"\n",
    "        cls._initialize_driver()  # Ensure the driver is initialized\n",
    "        try:\n",
    "            cls._driver.get(url)\n",
    "            time.sleep(5)  # Wait for the page to load\n",
    "            html_content = cls._driver.page_source\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            main_content = soup.find(['div', 'main', 'article'],\n",
    "                                      class_=re.compile(\n",
    "                    r'\\b(content|main-content|post-content|entry-content|article-body|'\n",
    "                    r'product-description|the-content|post-entry|entry|sqs-block-content|'\n",
    "                    r'content-wrapper|post-body|rich-text-section|postArticle-content|'\n",
    "                    r'post-full-content|item-description|message-body|thread-content|'\n",
    "                    r'story-content|news-article-body)\\b',\n",
    "                    re.IGNORECASE\n",
    "                )) or soup.body\n",
    "            main_text = main_content.get_text(separator=' ', strip=True) if main_content else ''\n",
    "            return re.sub(r'\\s+', ' ', main_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Selenium extraction failed for {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def quit_driver(cls):\n",
    "        \"\"\"Quits the WebDriver if it is running.\"\"\"\n",
    "        if cls._driver is None:\n",
    "            return\n",
    "        cls._driver.quit()\n",
    "        cls._driver = None  # Reset the driver to None after quitting\n",
    "        \n",
    "    @classmethod\n",
    "    def extract_content(cls, url: str) -> str:\n",
    "        \"\"\"Extracts content, handling dynamic content and fallbacks.\"\"\"\n",
    "        if not cls.is_valid_url(url):\n",
    "            logger.error(f\"Invalid URL: {url}\")\n",
    "            return \"\"\n",
    "        for extractor in [cls._extract_with_requests, cls._extract_with_newspaper, cls.extract_with_selenium]:\n",
    "            text = extractor(url)\n",
    "            if len(text.strip()) >= 200:\n",
    "                return text\n",
    "        return \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def _extract_with_requests(cls, url: str) -> str:\n",
    "        \"\"\"Extracts content using requests.\"\"\"\n",
    "        for attempt in range(1, cls.MAX_RETRIES + 1):\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': random.choice(cls.USER_AGENTS),\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "                    'Accept-Language': 'en-US,en;q=0.9',\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                    'Connection': 'keep-alive',\n",
    "                    'Upgrade-Insecure-Requests': '1',\n",
    "                    'Cache-Control': 'max-age=0',\n",
    "                    'DNT': '1',\n",
    "                }\n",
    "                response = requests.get(url, headers=headers, timeout=cls.TIMEOUT)\n",
    "                response.raise_for_status()\n",
    "                content_type = response.headers.get('Content-Type', '').lower()\n",
    "\n",
    "                if 'text/html' not in content_type:\n",
    "                    logger.warning(f\"Non-HTML content returned for {url}: {content_type}\")\n",
    "                    return \"\"\n",
    "                if response.headers.get('content-encoding') == 'gzip':\n",
    "                    try:\n",
    "                        html_content = gzip.decompress(response.content).decode('utf-8', errors='ignore')\n",
    "                    except (OSError, gzip.BadGzipFile) as e:\n",
    "                        logger.warning(f\"Error decoding gzip content: {e}. Using raw content.\")\n",
    "                        html_content = response.text\n",
    "                else:\n",
    "                    html_content = response.text\n",
    "\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                return cls._extract_content_from_soup(soup) \n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < cls.MAX_RETRIES:\n",
    "                    logger.warning(f\"Error with requests for {url} (attempt {attempt}): {e}. Retrying...\")\n",
    "                    time.sleep(2 ** attempt) \n",
    "                else:\n",
    "                    logger.warning(f\"Error with requests for {url} after {cls.MAX_RETRIES} attempts: {e}. Giving up.\")\n",
    "                    return \"\"  \n",
    "\n",
    "    @classmethod\n",
    "    def _extract_with_newspaper(cls, url: str) -> str:\n",
    "        \"\"\"Extracts content using newspaper3k.\"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            return article.text \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Newspaper error for {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def extract_with_selenium(cls, url: str) -> str:\n",
    "        \"\"\"Extracts content using Selenium (for dynamic content).\"\"\"\n",
    "        driver = cls.get_driver() \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # Wait for the body or a specific element\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            html_content = driver.page_source\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            main_content = soup.find(\n",
    "                ['div', 'main', 'article'],\n",
    "                class_=re.compile(\n",
    "                    r'\\b(content|main-content|post-content|entry-content|article-body|'\n",
    "                    r'product-description|the-content|post-entry|entry|sqs-block-content|'\n",
    "                    r'content-wrapper|post-body|rich-text-section|postArticle-content|'\n",
    "                    r'post-full-content|item-description|message-body|thread-content|'\n",
    "                    r'story-content|news-article-body)\\b',\n",
    "                    re.IGNORECASE\n",
    "                )\n",
    "            ) or soup.body\n",
    "            main_text = main_content.get_text(separator=' ', strip=True) if main_content else ''\n",
    "            return re.sub(r'\\s+', ' ', main_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Selenium extraction failed for {url}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_content_from_soup(soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Helper method to extract and clean content from BeautifulSoup object.\"\"\"\n",
    "        for element in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
    "            element.decompose()\n",
    "\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        content = soup.find('main') or soup.find('article') or soup.find(\n",
    "            'div', class_=re.compile(r'content|main-content|post-content|body|main-body|body-content|main', re.IGNORECASE))\n",
    "\n",
    "        if not content:\n",
    "            content = soup.body\n",
    "\n",
    "        if content:\n",
    "            h = html2text.HTML2Text()\n",
    "            h.ignore_links = True\n",
    "            h.ignore_images = True\n",
    "            text = h.handle(str(content))\n",
    "\n",
    "            text = re.sub(r'\\n+', '\\n', text)\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = text.strip()\n",
    "            return text\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_url(url: str) -> bool:\n",
    "        \"\"\"Checks if a URL is valid.\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "\n",
    "class SearchManager:\n",
    "    \"\"\"Manages searches across multiple APIs and providers with enhanced caching.\"\"\"\n",
    "\n",
    "    def __init__(self, apis: Optional[List[SearchAPI]] = None, \n",
    "                 web_search_provider: Optional[SearchProvider] = None,\n",
    "                 max_content_length: int = 10000, \n",
    "                 cache_size: int = 100, \n",
    "                 cache_ttl: int = 3600):\n",
    "        \"\"\"Initialize SearchManager with flexible configuration.\n",
    "        \n",
    "        Args:\n",
    "            apis: Optional list of SearchAPI instances\n",
    "            web_search_provider: Optional SearchProvider instance\n",
    "            max_content_length: Maximum length of extracted content\n",
    "            cache_size: Maximum number of cached results\n",
    "            cache_ttl: Cache time-to-live in seconds\n",
    "        \"\"\"\n",
    "        self.apis = apis or []\n",
    "        self.web_search_provider = web_search_provider or DuckDuckGoSearchProvider()\n",
    "        self.content_extractor = WebContentExtractor()\n",
    "        self.max_content_length = max_content_length\n",
    "        self.cache = {}\n",
    "        self.cache_timestamps = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.cache_ttl = cache_ttl\n",
    "        \n",
    "    def search(self, query: str, num_results: int = 10) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Performs a cached search using available APIs and the web search provider.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            num_results: Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing search results with metadata\n",
    "        \"\"\"\n",
    "        if not query.strip():\n",
    "            return []\n",
    "\n",
    "        cache_key = f\"{query}:{num_results}\"\n",
    "        cached_results = self._get_cached_results(cache_key)\n",
    "        if cached_results:\n",
    "            return cached_results\n",
    "\n",
    "        detailed_results = self._try_api_search(query, num_results)\n",
    "        \n",
    "        if not detailed_results:\n",
    "            logger.info(f\"Falling back to DuckDuckGo for query: {query}\")\n",
    "            duck_results = self.web_search_provider.search(query, num_results)\n",
    "            detailed_results = self._process_search_results(duck_results)\n",
    "\n",
    "        if detailed_results:\n",
    "            self._cache_results(cache_key, detailed_results)\n",
    "            \n",
    "        return detailed_results\n",
    "\n",
    "    def _get_cached_results(self, cache_key: str) -> Optional[List[Dict[str, str]]]:\n",
    "        \"\"\"Get results from cache if valid.\"\"\"\n",
    "        if cache_key in self.cache:\n",
    "            timestamp = self.cache_timestamps.get(cache_key)\n",
    "            if timestamp and (datetime.now() - timestamp) < timedelta(seconds=self.cache_ttl):\n",
    "                logger.info(f\"Returning cached results for key: {cache_key}\")\n",
    "                return self.cache[cache_key]\n",
    "        return None\n",
    "\n",
    "    def _try_api_search(self, query: str, num_results: int) -> Optional[List[Dict[str, str]]]:\n",
    "        \"\"\"Try searching using available APIs in order of preference.\"\"\"\n",
    "        api_order = [\"Google\", \"Brave\", \"DuckDuckGo\"]\n",
    "        \n",
    "        for api_name in api_order:\n",
    "            api = next((api for api in self.apis if api.name == api_name), None)\n",
    "            if api and api.is_within_quota():\n",
    "                try:\n",
    "                    logger.info(f\"Trying {api_name} for query: {query}\")\n",
    "                    search_results = api.search(query, num_results)\n",
    "                    if search_results:\n",
    "                        return self._process_search_results(search_results)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error searching {api_name}: {e}\")\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def _process_search_results(self, search_results: List[SearchResult]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Process search results and extract content safely.\"\"\"\n",
    "        detailed_results = []\n",
    "        for result in search_results:\n",
    "            try:\n",
    "                content = self.content_extractor.extract_content(result.url)\n",
    "                detailed_results.append({\n",
    "                    'title': result.title[:500],  # Limit title length\n",
    "                    'url': result.url[:1000],     # Limit URL length\n",
    "                    'snippet': result.snippet[:1000],  # Limit snippet length\n",
    "                    'content': content[:self.max_content_length] if content else \"\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing result {result.url}: {e}\")\n",
    "                continue\n",
    "        return detailed_results\n",
    "\n",
    "    def _cache_results(self, cache_key: str, results: List[Dict]):\n",
    "        \"\"\"Cache search results with timestamp.\"\"\"\n",
    "        self.cache[cache_key] = results\n",
    "        self.cache_timestamps[cache_key] = datetime.now()\n",
    "        \n",
    "        # Remove oldest entries if cache is full\n",
    "        while len(self.cache) > self.cache_size:\n",
    "            oldest_key = min(self.cache_timestamps.items(), key=lambda x: x[1])[0]\n",
    "            del self.cache[oldest_key]\n",
    "            del self.cache_timestamps[oldest_key]\n",
    "\n",
    "    def clear_expired_cache(self):\n",
    "        \"\"\"Clear expired cache entries.\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        expired_keys = [\n",
    "            key for key, timestamp in self.cache_timestamps.items()\n",
    "            if (current_time - timestamp).total_seconds() > self.cache_ttl\n",
    "        ]\n",
    "        for key in expired_keys:\n",
    "            del self.cache[key]\n",
    "            del self.cache_timestamps[key]\n",
    "\n",
    "\n",
    "\n",
    "def initialize_search_manager() -> SearchManager:\n",
    "    \"\"\"Initialize SearchManager with default configuration.\"\"\"\n",
    "    try:\n",
    "        apis = []\n",
    "        if GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID:\n",
    "            apis.append(SearchAPI(\n",
    "                \"Google\",\n",
    "                GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY,\n",
    "                \"https://www.googleapis.com/customsearch/v1\",\n",
    "                {\"cx\": GOOGLE_CUSTOM_SEARCH_ENGINE_ID},\n",
    "                100,\n",
    "                'items',\n",
    "                1\n",
    "            ))\n",
    "        \n",
    "        if BRAVE_SEARCH_API_KEY:\n",
    "            apis.append(SearchAPI(\n",
    "                \"Brave\",\n",
    "                BRAVE_SEARCH_API_KEY,\n",
    "                \"https://api.search.brave.com/res/v1/web/search\",\n",
    "                {},\n",
    "                2000,\n",
    "                'results',\n",
    "                1\n",
    "            ))\n",
    "\n",
    "        # DuckDuckGo is always available as fallback\n",
    "        web_search_provider = DuckDuckGoSearchProvider()\n",
    "        \n",
    "        return SearchManager(\n",
    "            apis=apis,\n",
    "            web_search_provider=web_search_provider,\n",
    "            max_content_length=10000,\n",
    "            cache_size=100,\n",
    "            cache_ttl=3600\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing SearchManager: {e}\")\n",
    "        # Return a SearchManager with just DuckDuckGo as fallback\n",
    "        return SearchManager(\n",
    "            apis=[],\n",
    "            web_search_provider=DuckDuckGoSearchProvider()\n",
    "        )\n",
    "\n",
    "\n",
    "# Example tool function (from your description)\n",
    "def foia_search(query: str) -> List[str]:\n",
    "    \"\"\"Searches FOIA.gov for the given query and returns a list of relevant content.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text content extracted from relevant FOIA.gov search results.\n",
    "    \"\"\"\n",
    "    url = f\"https://search.foia.gov/search?utf8=%E2%9C%93&m=true&affiliate=foia.gov&query={query.replace(' ', '+')}\"\n",
    "    web_content_extractor = WebContentExtractor()\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(web_content_extractor.USER_AGENTS),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'DNT': '1',\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=web_content_extractor.TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract links to actual FOIA results (not navigation links)\n",
    "        result_links = [a['href'] for a in soup.select('.result-title a') if a.has_attr('href')]\n",
    "\n",
    "        content = []\n",
    "        for link in result_links:\n",
    "            try:\n",
    "                if extracted_content := WebContentExtractor.extract_content(\n",
    "                    link\n",
    "                ):\n",
    "                    content.append(extracted_content)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting content from {link}: {e}\")\n",
    "\n",
    "        return content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error searching FOIA.gov: {e}\")\n",
    "        return []\n",
    "\n",
    "num_results = 10\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    search_manager = initialize_search_manager()\n",
    "    query = \"test\"\n",
    "    num_results = 15\n",
    "\n",
    "    if search_manager:\n",
    "        results = search_manager.search(query, num_results)\n",
    "        for result in results:\n",
    "            print(f\"Title: {result['title']}\")\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Snippet: {result['snippet']}\")\n",
    "            print(f\"Content: {result['content'][:15000]}...\")  \n",
    "            print(\"---\")\n",
    "    else:\n",
    "        print(\"Search functionality is disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\search_manager.py\n",
    "\n",
    "from certifi import contents\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from fake_useragent import UserAgent\n",
    "import html2text\n",
    "from duckduckgo_search import DDGS\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import gzip\n",
    "from utils import log_and_handle_error\n",
    "from newspaper import Article\n",
    "from functools import lru_cache\n",
    "from datetime import datetime, timedelta\n",
    "from selenium_stealth import stealth\n",
    "import asyncio\n",
    "from .web_search import SearchAPI, SearchResult, DuckDuckGoSearchProvider, initialize_apis\n",
    "from .content_extractor import WebContentExtractor\n",
    "\n",
    "\n",
    "def fetch_article_text(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    title = article.title\n",
    "    author = ', '.join(article.authors)\n",
    "    pub_date = article.publish_date\n",
    "    article_text = article.text\n",
    "    \n",
    "    return title, author, pub_date, article_text\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Google API keys\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_ID = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_ID')\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY= os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY')\n",
    "BRAVE_SEARCH_API_KEY = os.getenv('BRAVE_SEARCH_API_KEY')  # Brave Search API key (if available)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)  # Get a logger instance\n",
    "\n",
    "class SearchManager:\n",
    "    \"\"\"Manages searches across multiple APIs and providers with enhanced caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, apis: Optional[List[SearchAPI]] = None,\n",
    "                 web_search_provider: Optional[DuckDuckGoSearchProvider] = None,\n",
    "                 max_content_length: int = 10000,\n",
    "                 cache_size: int = 100,\n",
    "                 cache_ttl: int = 3600):\n",
    "        \"\"\"Initialize SearchManager with flexible configuration.\"\"\"\n",
    "        self.apis = apis or []\n",
    "        self.web_search_provider = web_search_provider or DuckDuckGoSearchProvider()\n",
    "        self.content_extractor = WebContentExtractor()\n",
    "        self.max_content_length = max_content_length\n",
    "        self.cache = {}\n",
    "        self.cache_timestamps = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.cache_ttl = cache_ttl\n",
    "    \n",
    "    def _get_cached_results(self, cache_key: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Get results from cache if valid.\"\"\"\n",
    "        if cache_key in self.cache:\n",
    "            timestamp = self.cache_timestamps.get(cache_key, 0)\n",
    "            if time.time() - timestamp <= self.cache_ttl:\n",
    "                return self.cache[cache_key]\n",
    "        return None\n",
    "    \n",
    "    async def _try_api_search(self, query: str, num_results: int) -> List[SearchResult]:\n",
    "        \"\"\"Try searching using available APIs in order of preference.\"\"\"\n",
    "        for api in self.apis:\n",
    "            if await api.is_within_quota():\n",
    "                results = await api.search(query, num_results)\n",
    "                if results:\n",
    "                    return results\n",
    "        \n",
    "        # Fallback to DuckDuckGo if all APIs fail or are over quota\n",
    "        return await self.web_search_provider.search(query, num_results)\n",
    "    \n",
    "    def _process_search_results(self, search_results: List[SearchResult]) -> List[Dict]:\n",
    "        \"\"\"Process search results and extract content safely.\"\"\"\n",
    "        processed_results = []\n",
    "        for result in search_results:\n",
    "            try:\n",
    "                content = self.content_extractor.extract_content(result.url)\n",
    "                if content:\n",
    "                    # Truncate content if it's too long\n",
    "                    content = content[:self.max_content_length]\n",
    "                    result.content = content\n",
    "                processed_results.append(result.to_dict())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing result {result.url}: {e}\")\n",
    "        return processed_results\n",
    "    \n",
    "    def _cache_results(self, cache_key: str, results: List[Dict]):\n",
    "        \"\"\"Cache search results with timestamp.\"\"\"\n",
    "        self.cache[cache_key] = results\n",
    "        self.cache_timestamps[cache_key] = time.time()\n",
    "        \n",
    "        # Remove oldest entries if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest_key = min(self.cache_timestamps.keys(),\n",
    "                           key=lambda k: self.cache_timestamps[k])\n",
    "            del self.cache[oldest_key]\n",
    "            del self.cache_timestamps[oldest_key]\n",
    "    \n",
    "    def clear_expired_cache(self):\n",
    "        \"\"\"Clear expired cache entries.\"\"\"\n",
    "        current_time = time.time()\n",
    "        expired_keys = [k for k, v in self.cache_timestamps.items()\n",
    "                       if current_time - v > self.cache_ttl]\n",
    "        for key in expired_keys:\n",
    "            del self.cache[key]\n",
    "            del self.cache_timestamps[key]\n",
    "    \n",
    "    async def search(self, query: str, num_results: int = 10) -> List[Dict]:\n",
    "        \"\"\"Performs a cached search using available APIs and the web search provider.\"\"\"\n",
    "        cache_key = f\"{query}:{num_results}\"\n",
    "        \n",
    "        # Try to get results from cache\n",
    "        cached_results = self._get_cached_results(cache_key)\n",
    "        if cached_results is not None:\n",
    "            return cached_results\n",
    "        \n",
    "        # Perform new search\n",
    "        search_results = await self._try_api_search(query, num_results)\n",
    "        processed_results = self._process_search_results(search_results)\n",
    "        \n",
    "        # Cache the results\n",
    "        self._cache_results(cache_key, processed_results)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "def configure_search_settings() -> Dict[str, Any]:\n",
    "    \"\"\"Prompts the user to enable/disable search functionality.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Do you want to enable search functionality? (Y/N): \").strip().lower()\n",
    "            if user_input == 'y':\n",
    "                return {\n",
    "                    'search_enabled': True,\n",
    "                    'all_search_result_data': {},\n",
    "                    'search_session_counter': 0,\n",
    "                    'search_session_id': 0,\n",
    "                    'apis': initialize_apis(),\n",
    "                }\n",
    "            elif user_input == 'n':\n",
    "                return {'search_enabled': False}\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter Y or N.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}. Please try again.\")\n",
    "\n",
    "async def initialize_search_manager() -> SearchManager:\n",
    "    \"\"\"Initialize SearchManager with default configuration.\"\"\"\n",
    "    try:\n",
    "        apis = []\n",
    "        if GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID:\n",
    "            apis.append(SearchAPI(\n",
    "                \"Google\",\n",
    "                GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY,\n",
    "                \"https://www.googleapis.com/customsearch/v1\",\n",
    "                {\"cx\": GOOGLE_CUSTOM_SEARCH_ENGINE_ID},\n",
    "                100,\n",
    "                'items',\n",
    "                1\n",
    "            ))\n",
    "        \n",
    "        if BRAVE_SEARCH_API_KEY:\n",
    "            apis.append(SearchAPI(\n",
    "                \"Brave\",\n",
    "                BRAVE_SEARCH_API_KEY,\n",
    "                \"https://api.search.brave.com/res/v1/web/search\",\n",
    "                {},\n",
    "                2000,\n",
    "                'results',\n",
    "                1\n",
    "            ))\n",
    "\n",
    "        # DuckDuckGo is always available as fallback\n",
    "        web_search_provider = DuckDuckGoSearchProvider()\n",
    "        \n",
    "        return SearchManager(\n",
    "            apis=apis,\n",
    "            web_search_provider=web_search_provider,\n",
    "            max_content_length=10000,\n",
    "            cache_size=100,\n",
    "            cache_ttl=3600\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing SearchManager: {e}\")\n",
    "        # Return a SearchManager with just DuckDuckGo as fallback\n",
    "        return SearchManager(\n",
    "            apis=[],\n",
    "            web_search_provider=DuckDuckGoSearchProvider()\n",
    "        )\n",
    "\n",
    "\n",
    "# Example tool function (from your description)\n",
    "async def foia_search(query: str) -> List[str]:\n",
    "    \"\"\"Searches FOIA.gov for the given query and returns a list of relevant content.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text content extracted from relevant FOIA.gov search results.\n",
    "    \"\"\"\n",
    "    url = f\"https://search.foia.gov/search?utf8=%E2%9C%93&m=true&affiliate=foia.gov&query={query.replace(' ', '+')}\"\n",
    "    web_content_extractor = WebContentExtractor()\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(web_content_extractor.USER_AGENTS),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'DNT': '1',\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=web_content_extractor.TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract links to actual FOIA results (not navigation links)\n",
    "        result_links = [a['href'] for a in soup.select('.result-title a') if a.has_attr('href')]\n",
    "\n",
    "        content = []\n",
    "        for link in result_links:\n",
    "            try:\n",
    "                if extracted_content := await WebContentExtractor.extract_content(\n",
    "                    link\n",
    "                ):\n",
    "                    content.append(extracted_content)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting content from {link}: {e}\")\n",
    "\n",
    "        return content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error searching FOIA.gov: {e}\")\n",
    "        return []\n",
    "\n",
    "num_results = 10\n",
    "\n",
    "# Example usage\n",
    "async def main():\n",
    "    search_manager = await initialize_search_manager()\n",
    "    query = \"test\"\n",
    "    num_results = 15\n",
    "\n",
    "    if search_manager:\n",
    "        results = await search_manager.search(query, num_results)\n",
    "        for result in results:\n",
    "            print(f\"Title: {result['title']}\")\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Snippet: {result['snippet']}\")\n",
    "            print(f\"Content: {result['content'][:15000]}...\")  \n",
    "            print(\"---\")\n",
    "    else:\n",
    "        print(\"Search functionality is disabled.\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\search_tool.py\n",
    "\n",
    "\"\"\"Web search tool implementation.\"\"\"\n",
    "from typing import Dict, Any\n",
    "from .base_tool import BaseTool, ToolResult\n",
    "from ..config import TIMEOUT\n",
    "\n",
    "class SearchTool(BaseTool):\n",
    "    \"\"\"Tool for performing web searches.\"\"\"\n",
    "    \n",
    "    def __init__(self, search_manager):\n",
    "        super().__init__(\n",
    "            name=\"web_search\",\n",
    "            description=\"Performs web searches and returns relevant results\"\n",
    "        )\n",
    "        self.search_manager = search_manager\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The search query\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"num_results\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Maximum number of results to return\",\n",
    "                \"default\": 10,\n",
    "                \"minimum\": 1\n",
    "            },\n",
    "            \"timeout_seconds\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Maximum time to wait for results\",\n",
    "                \"default\": TIMEOUT\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def execute(self, **kwargs) -> ToolResult:\n",
    "        try:\n",
    "            query = kwargs.get(\"query\")\n",
    "            if not query:\n",
    "                return ToolResult(\n",
    "                    success=False,\n",
    "                    result=None,\n",
    "                    error=\"Query parameter is required\"\n",
    "                )\n",
    "\n",
    "            num_results = kwargs.get(\"num_results\", 10)\n",
    "            timeout_seconds = kwargs.get(\"timeout_seconds\", TIMEOUT)\n",
    "\n",
    "            results = self.search_manager.search(\n",
    "                query,\n",
    "                num_results=num_results,\n",
    "                timeout=timeout_seconds\n",
    "            )\n",
    "\n",
    "            return ToolResult(\n",
    "                success=True,\n",
    "                result=results\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=str(e)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99832d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\search_tools.py\n",
    "\n",
    "\"\"\"Tools for specialized search functionality.\"\"\"\n",
    "from typing import Dict, Any, List, Optional\n",
    "from .base_tool import BaseTool, ToolResult\n",
    "from .specialized_search import FOIASearchProvider, ArXivSearchProvider\n",
    "import asyncio\n",
    "\n",
    "class FOIASearchTool(BaseTool):\n",
    "    \"\"\"Tool for searching FOIA.gov records.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"foia_search\",\n",
    "            description=\"Search FOIA.gov for government records\",\n",
    "            use_case=\"Use this tool to search for Freedom of Information Act (FOIA) records and documents.\",\n",
    "            operation=\"Searches FOIA.gov's database and returns relevant documents with their content.\"\n",
    "        )\n",
    "        self.provider = FOIASearchProvider()\n",
    "    \n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"query\": {\n",
    "                \"type\": str,\n",
    "                \"description\": \"The search query\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"max_results\": {\n",
    "                \"type\": int,\n",
    "                \"description\": \"Maximum number of results to return\",\n",
    "                \"required\": False,\n",
    "                \"default\": 10\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute(self, **kwargs) -> ToolResult:\n",
    "        try:\n",
    "            query = kwargs[\"query\"]\n",
    "            max_results = kwargs.get(\"max_results\", 10)\n",
    "            \n",
    "            # Run the async search\n",
    "            results = asyncio.run(self.provider.search(query, max_results))\n",
    "            \n",
    "            # Format results for output\n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                formatted_results.append({\n",
    "                    \"title\": result.title,\n",
    "                    \"url\": result.url,\n",
    "                    \"snippet\": result.snippet,\n",
    "                    \"content\": result.content\n",
    "                })\n",
    "            \n",
    "            return ToolResult(\n",
    "                success=True,\n",
    "                result=formatted_results\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=f\"FOIA search failed: {str(e)}\"\n",
    "            )\n",
    "\n",
    "class ArXivSearchTool(BaseTool):\n",
    "    \"\"\"Tool for searching arXiv papers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"arxiv_search\",\n",
    "            description=\"Search arXiv for scientific papers\",\n",
    "            use_case=\"Use this tool to search for scientific papers on arXiv.\",\n",
    "            operation=\"Searches arXiv's database and returns papers with their abstracts and metadata.\"\n",
    "        )\n",
    "        self.provider = ArXivSearchProvider()\n",
    "    \n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"query\": {\n",
    "                \"type\": str,\n",
    "                \"description\": \"The search query\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"max_results\": {\n",
    "                \"type\": int,\n",
    "                \"description\": \"Maximum number of results to return\",\n",
    "                \"required\": False,\n",
    "                \"default\": 10\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute(self, **kwargs) -> ToolResult:\n",
    "        try:\n",
    "            query = kwargs[\"query\"]\n",
    "            max_results = kwargs.get(\"max_results\", 10)\n",
    "            \n",
    "            # Run the async search\n",
    "            results = asyncio.run(self.provider.search(query, max_results))\n",
    "            \n",
    "            # Format results for output\n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                formatted_results.append({\n",
    "                    \"title\": result.title,\n",
    "                    \"url\": result.url,\n",
    "                    \"snippet\": result.snippet,\n",
    "                    \"content\": result.content\n",
    "                })\n",
    "            \n",
    "            return ToolResult(\n",
    "                success=True,\n",
    "                result=formatted_results\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=f\"arXiv search failed: {str(e)}\"\n",
    "            )\n",
    "\n",
    "class ArXivLatestTool(BaseTool):\n",
    "    \"\"\"Tool for fetching latest arXiv papers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"arxiv_latest\",\n",
    "            description=\"Get latest papers from arXiv\",\n",
    "            use_case=\"Use this tool to get the most recent papers from arXiv, optionally filtered by category.\",\n",
    "            operation=\"Fetches the latest papers from arXiv within a specified timeframe and category.\"\n",
    "        )\n",
    "        self.provider = ArXivSearchProvider()\n",
    "    \n",
    "    @property\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"category\": {\n",
    "                \"type\": str,\n",
    "                \"description\": \"arXiv category (e.g., 'cs.AI', 'physics')\",\n",
    "                \"required\": False,\n",
    "                \"default\": None\n",
    "            },\n",
    "            \"max_results\": {\n",
    "                \"type\": int,\n",
    "                \"description\": \"Maximum number of results to return\",\n",
    "                \"required\": False,\n",
    "                \"default\": 10\n",
    "            },\n",
    "            \"days\": {\n",
    "                \"type\": int,\n",
    "                \"description\": \"Number of past days to search\",\n",
    "                \"required\": False,\n",
    "                \"default\": 7\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute(self, **kwargs) -> ToolResult:\n",
    "        try:\n",
    "            category = kwargs.get(\"category\")\n",
    "            max_results = kwargs.get(\"max_results\", 10)\n",
    "            days = kwargs.get(\"days\", 7)\n",
    "            \n",
    "            # Run the async search\n",
    "            results = asyncio.run(self.provider.get_latest_papers(\n",
    "                category=category,\n",
    "                max_results=max_results,\n",
    "                days=days\n",
    "            ))\n",
    "            \n",
    "            # Format results for output\n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                formatted_results.append({\n",
    "                    \"title\": result.title,\n",
    "                    \"url\": result.url,\n",
    "                    \"snippet\": result.snippet,\n",
    "                    \"content\": result.content\n",
    "                })\n",
    "            \n",
    "            return ToolResult(\n",
    "                success=True,\n",
    "                result=formatted_results\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ToolResult(\n",
    "                success=False,\n",
    "                result=None,\n",
    "                error=f\"arXiv latest papers fetch failed: {str(e)}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\specialized_search.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "from .content_extractor import WebContentExtractor\n",
    "from .web_search import SearchResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FOIASearchProvider:\n",
    "    \"\"\"Provider for searching FOIA.gov records.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://www.foia.gov/api/search\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.content_extractor = WebContentExtractor()\n",
    "    \n",
    "    async def search(self, query: str, max_results: int = 10) -> List[SearchResult]:\n",
    "        \"\"\"Search FOIA.gov for records matching the query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            max_results (int): Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[SearchResult]: List of search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # FOIA.gov API parameters\n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"size\": max_results,\n",
    "                \"from\": 0,\n",
    "                \"sort\": \"relevance\",\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.BASE_URL, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = []\n",
    "            for item in data.get(\"results\", [])[:max_results]:\n",
    "                title = item.get(\"title\", \"No Title\")\n",
    "                url = item.get(\"url\") or f\"https://www.foia.gov/request/{item.get('id')}\"\n",
    "                snippet = item.get(\"description\", \"No description available\")\n",
    "                \n",
    "                # Try to extract content if URL is available\n",
    "                content = \"\"\n",
    "                if \"url\" in item:\n",
    "                    content = self.content_extractor.extract_content(item[\"url\"]) or \"\"\n",
    "                \n",
    "                results.append(SearchResult(\n",
    "                    title=title,\n",
    "                    url=url,\n",
    "                    snippet=snippet,\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching FOIA.gov: {e}\")\n",
    "            return []\n",
    "\n",
    "class ArXivSearchProvider:\n",
    "    \"\"\"Provider for searching arXiv papers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = arxiv.Client()\n",
    "    \n",
    "    async def get_latest_papers(self, category: str = None, max_results: int = 10,\n",
    "                              days: int = 7) -> List[SearchResult]:\n",
    "        \"\"\"Get the latest arXiv papers, optionally filtered by category.\n",
    "        \n",
    "        Args:\n",
    "            category (str, optional): arXiv category (e.g., 'cs.AI', 'physics')\n",
    "            max_results (int): Maximum number of results to return\n",
    "            days (int): Number of past days to search\n",
    "            \n",
    "        Returns:\n",
    "            List[SearchResult]: List of search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate date range\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=days)\n",
    "            \n",
    "            # Build search query\n",
    "            search_query = f\"submittedDate:[{start_date.strftime('%Y%m%d')}* TO {end_date.strftime('%Y%m%d')}*]\"\n",
    "            if category:\n",
    "                search_query += f\" AND cat:{category}\"\n",
    "            \n",
    "            # Create search\n",
    "            search = arxiv.Search(\n",
    "                query=search_query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "                sort_order=arxiv.SortOrder.Descending\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for paper in self.client.results(search):\n",
    "                # Extract authors\n",
    "                authors = \", \".join([author.name for author in paper.authors])\n",
    "                \n",
    "                # Create content combining abstract and metadata\n",
    "                content = f\"Title: {paper.title}\\n\\n\"\n",
    "                content += f\"Authors: {authors}\\n\\n\"\n",
    "                content += f\"Published: {paper.published}\\n\"\n",
    "                content += f\"Updated: {paper.updated}\\n\"\n",
    "                content += f\"DOI: {paper.doi}\\n\" if paper.doi else \"\"\n",
    "                content += f\"Primary Category: {paper.primary_category}\\n\"\n",
    "                content += f\"Categories: {', '.join(paper.categories)}\\n\\n\"\n",
    "                content += f\"Abstract:\\n{paper.summary}\\n\\n\"\n",
    "                content += f\"PDF URL: {paper.pdf_url}\\n\"\n",
    "                \n",
    "                results.append(SearchResult(\n",
    "                    title=paper.title,\n",
    "                    url=paper.entry_id,\n",
    "                    snippet=paper.summary[:200] + \"...\",\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching arXiv papers: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def search(self, query: str, max_results: int = 10) -> List[SearchResult]:\n",
    "        \"\"\"Search arXiv papers by query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            max_results (int): Maximum number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[SearchResult]: List of search results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for paper in self.client.results(search):\n",
    "                # Extract authors\n",
    "                authors = \", \".join([author.name for author in paper.authors])\n",
    "                \n",
    "                # Create content combining abstract and metadata\n",
    "                content = f\"Title: {paper.title}\\n\\n\"\n",
    "                content += f\"Authors: {authors}\\n\\n\"\n",
    "                content += f\"Published: {paper.published}\\n\"\n",
    "                content += f\"Updated: {paper.updated}\\n\"\n",
    "                content += f\"DOI: {paper.doi}\\n\" if paper.doi else \"\"\n",
    "                content += f\"Primary Category: {paper.primary_category}\\n\"\n",
    "                content += f\"Categories: {', '.join(paper.categories)}\\n\\n\"\n",
    "                content += f\"Abstract:\\n{paper.summary}\\n\\n\"\n",
    "                content += f\"PDF URL: {paper.pdf_url}\\n\"\n",
    "                \n",
    "                results.append(SearchResult(\n",
    "                    title=paper.title,\n",
    "                    url=paper.entry_id,\n",
    "                    snippet=paper.summary[:200] + \"...\",\n",
    "                    content=content\n",
    "                ))\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching arXiv: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\tool_manager (2).py\n",
    "\n",
    "\"\"\"Tool management functionality.\"\"\"\n",
    "import logging\n",
    "from typing import Callable, Dict, Any, List\n",
    "from .search_manager import SearchManager\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ToolManager:\n",
    "    def __init__(self, search_manager: SearchManager = None):\n",
    "        self.search_manager = search_manager\n",
    "        self.tools = {}\n",
    "\n",
    "    def register_tool(self, name: str, tool_func: Callable):\n",
    "        self.tools[name] = tool_func\n",
    "\n",
    "    def execute_tool(self, name: str, *args, **kwargs):\n",
    "        if name not in self.tools:\n",
    "            raise ValueError(f\"Tool '{name}' not found.\")\n",
    "        return self.tools[name](*args, **kwargs)\n",
    "\n",
    "    def list_tools(self) -> List[str]:\n",
    "        return list(self.tools.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\tool_manager.py\n",
    "\n",
    "\"\"\"\n",
    "Tool manager for creating and managing different tools.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, List, Optional, Type, Union\n",
    "from .base.tool import BaseTool\n",
    "from .core.search_tool import SearchTool\n",
    "from .core.code_analysis_tool import CodeAnalysisTool\n",
    "from langchain.tools import BaseTool as LangChainTool\n",
    "from .llm_tools import get_llm_tools\n",
    "\n",
    "class ToolManager:\n",
    "    \"\"\"Manages the creation and lifecycle of tools.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, BaseTool] = {}\n",
    "        self._tool_classes: Dict[str, Type[BaseTool]] = {}\n",
    "        self._llm_tools: List[LangChainTool] = []\n",
    "        self.register_tools()\n",
    "        \n",
    "    def register_tools(self):\n",
    "        \"\"\"Register all available tools.\"\"\"\n",
    "        from .search_tools import FOIASearchTool, ArXivSearchTool, ArXivLatestTool\n",
    "        from .core.search_tool import SearchTool\n",
    "        from .core.code_analysis_tool import CodeAnalysisTool\n",
    "        \n",
    "        # Register traditional tools\n",
    "        self._tool_classes.update({\n",
    "            \"web_search\": SearchTool,\n",
    "            \"code_analysis\": CodeAnalysisTool,\n",
    "            \"foia_search\": FOIASearchTool,\n",
    "            \"arxiv_search\": ArXivSearchTool,\n",
    "            \"arxiv_latest\": ArXivLatestTool\n",
    "        })\n",
    "        \n",
    "        # Register LLM tools\n",
    "        self._llm_tools.extend(get_llm_tools())\n",
    "    \n",
    "    def get_llm_tools(self) -> List[LangChainTool]:\n",
    "        \"\"\"Get all registered LLM tools.\"\"\"\n",
    "        return self._llm_tools\n",
    "    \n",
    "    def get_traditional_tools(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get all registered traditional tools.\"\"\"\n",
    "        return self._tool_classes\n",
    "    \n",
    "    def get_tool(self, tool_type: str) -> Optional[Union[Any, LangChainTool]]:\n",
    "        \"\"\"Get a tool by type.\"\"\"\n",
    "        # First check traditional tools\n",
    "        if tool_type in self._tool_classes:\n",
    "            return self._tool_classes[tool_type]()\n",
    "        \n",
    "        # Then check LLM tools\n",
    "        for tool in self._llm_tools:\n",
    "            if tool.__name__ == tool_type:\n",
    "                return tool\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def create_tool(\n",
    "        self,\n",
    "        tool_type: str,\n",
    "        tool_config: Optional[Dict[str, Any]] = None\n",
    "    ) -> BaseTool:\n",
    "        \"\"\"\n",
    "        Create a new tool instance.\n",
    "        \n",
    "        Args:\n",
    "            tool_type: Type of tool to create\n",
    "            tool_config: Optional tool configuration\n",
    "            \n",
    "        Returns:\n",
    "            Created tool instance\n",
    "        \"\"\"\n",
    "        if tool_type not in self._tool_classes:\n",
    "            raise ValueError(f\"Unknown tool type: {tool_type}\")\n",
    "            \n",
    "        tool_class = self._tool_classes[tool_type]\n",
    "        tool = tool_class(tool_config)\n",
    "        \n",
    "        # Store tool instance for reuse\n",
    "        self.tools[tool_type] = tool\n",
    "        return tool\n",
    "        \n",
    "    def list_tools(self) -> Dict[str, str]:\n",
    "        \"\"\"List all available tool types.\"\"\"\n",
    "        return {name: tool.description for name, tool in self.tools.items()}\n",
    "        \n",
    "    def get_tools_for_agent(self, agent_type: str) -> List[str]:\n",
    "        \"\"\"Get list of tools available to a specific agent type.\"\"\"\n",
    "        # This could be expanded to include agent-specific tool restrictions\n",
    "        return list(self._tool_classes.keys())\n",
    "        \n",
    "    def execute_tool(self, tool_type: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute a tool with the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            tool_type: Type of tool to execute\n",
    "            params: Tool parameters\n",
    "            \n",
    "        Returns:\n",
    "            Tool execution results\n",
    "        \"\"\"\n",
    "        tool = self.get_tool(tool_type)\n",
    "        if not tool:\n",
    "            if tool_type in self._tool_classes:\n",
    "                tool = self.create_tool(tool_type)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown tool type: {tool_type}\")\n",
    "            \n",
    "        return tool.execute(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\tools.py\n",
    "\n",
    "\"\"\"Module for defining and managing external tools for the AI assistant.\"\"\"\n",
    "from typing import List, Annotated, Optional\n",
    "from search_manager import SearchManager\n",
    "from agent_tools.fetch_latest_arxiv_papers import fetch_latest_arxiv_results\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds: int):\n",
    "    \"\"\"Context manager for timing out operations after specified seconds.\"\"\"\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutError(\"Operation timed out\")\n",
    "    \n",
    "    # Register a function to raise a TimeoutError on the signal\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Disable the alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def web_search(\n",
    "    search_manager: SearchManager, \n",
    "    query: str, \n",
    "    num_results: int = 10,\n",
    "    timeout_seconds: int = 30\n",
    ") -> str:\n",
    "    \"\"\"Performs a web search using the provided SearchManager.\n",
    "            \n",
    "    Args:\n",
    "        search_manager: The SearchManager instance to use.\n",
    "        query (str): The search query.\n",
    "        num_results (int, optional): The maximum number of results to return. Defaults to 10.\n",
    "        timeout_seconds (int, optional): Maximum time to wait for search results. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the search results.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If num_results is less than 1 or query is empty.\n",
    "        TimeoutError: If the search operation times out.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        raise ValueError(\"Search query cannot be empty\")\n",
    "    \n",
    "    if num_results < 1:\n",
    "        raise ValueError(\"num_results must be at least 1\")\n",
    "\n",
    "    try:\n",
    "        with timeout(timeout_seconds):\n",
    "            results = search_manager.search(query, num_results)\n",
    "            if not results:\n",
    "                return \"No results found for the given query.\"\n",
    "            \n",
    "            return \"\\n\\n\".join(\n",
    "                [\n",
    "                    f\"**{result['title']}** ({result['url']})\\n{result['snippet']}\\n{result['content'][:50000]}\"\n",
    "                    for result in results\n",
    "                ]\n",
    "            )\n",
    "    except TimeoutError:\n",
    "        return \"Search operation timed out. Please try again or refine your query.\"\n",
    "    except Exception as e:\n",
    "        return f\"Search failed: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def fetch_recent_arxiv_papers_by_topic(\n",
    "    topic: str,\n",
    "    timeout_seconds: int = 30\n",
    ") -> List[str]:\n",
    "    \"\"\"Fetches recent arXiv papers based on a given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic (str): The topic to search for papers.\n",
    "        timeout_seconds (int, optional): Maximum time to wait for results. Defaults to 30.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of paper information.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If topic is empty.\n",
    "        TimeoutError: If the operation times out.\n",
    "    \"\"\"\n",
    "    if not topic.strip():\n",
    "        raise ValueError(\"Topic cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        with timeout(timeout_seconds):\n",
    "            return fetch_latest_arxiv_results(topic)\n",
    "    except TimeoutError:\n",
    "        return [\"Operation timed out while fetching arXiv papers. Please try again.\"]\n",
    "    except Exception as e:\n",
    "        return [f\"Failed to fetch arXiv papers: {str(e)}\"]\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "    timeout_seconds: int = 10,\n",
    "    max_output_length: int = 10000\n",
    ") -> str:\n",
    "    \"\"\"Executes Python code and returns the output.\n",
    "    \n",
    "    Args:\n",
    "        code (str): The Python code to execute.\n",
    "        timeout_seconds (int, optional): Maximum execution time in seconds. Defaults to 10.\n",
    "        max_output_length (int, optional): Maximum length of output to return. Defaults to 10000.\n",
    "        \n",
    "    Returns:\n",
    "        str: The execution result or error message.\n",
    "        \n",
    "    Raises:\n",
    "        TimeoutError: If code execution exceeds timeout_seconds.\n",
    "    \"\"\"\n",
    "    if not code.strip():\n",
    "        return \"No code provided to execute.\"\n",
    "\n",
    "    try:\n",
    "        with timeout(timeout_seconds):\n",
    "            start_time = time.time()\n",
    "            result = repl.run(code)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            if len(str(result)) > max_output_length:\n",
    "                result = str(result)[:max_output_length] + \"... (output truncated)\"\n",
    "            \n",
    "            return (\n",
    "                f\"Successfully executed in {execution_time:.2f}s:\\n\"\n",
    "                f\"```python\\n{code}\\n```\\n\"\n",
    "                f\"Stdout: {result}\"\n",
    "            )\n",
    "    except TimeoutError:\n",
    "        return f\"Code execution timed out after {timeout_seconds} seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\\web_search.py\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from fake_useragent import UserAgent\n",
    "from duckduckgo_search import DDGS\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Google API keys\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_ID = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_ID')\n",
    "GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY = os.getenv('GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY')\n",
    "BRAVE_SEARCH_API_KEY = os.getenv('BRAVE_SEARCH_API_KEY')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SearchProvider(ABC):\n",
    "    \"\"\"Abstract base class for search providers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def search(self, query: str, num_results: int) -> List['SearchResult']:\n",
    "        \"\"\"Perform a search and return a list of SearchResult objects.\"\"\"\n",
    "        pass\n",
    "\n",
    "class SearchResult:\n",
    "    \"\"\"Represents a single search result.\"\"\"\n",
    "    \n",
    "    def __init__(self, title: str, url: str, snippet: str, content: str = \"\"):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.snippet = snippet\n",
    "        self.content = content\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, str]):\n",
    "        return cls(data['title'], data['url'], data['snippet'], data['content'])\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'title': self.title,\n",
    "            'url': self.url,\n",
    "            'snippet': self.snippet,\n",
    "            'content': self.content\n",
    "        }\n",
    "\n",
    "class SearchAPI(SearchProvider):\n",
    "    \"\"\"Represents a search API with rate limiting and quota management.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, api_key: str, base_url: str, params: dict, quota: int, results_path: str,\n",
    "                 rate_limit: int):\n",
    "        self.name = name\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.params = params.copy()\n",
    "        if api_key:\n",
    "            self.params['key'] = api_key\n",
    "        self.quota = quota\n",
    "        self.used = 0\n",
    "        self.results_path = results_path\n",
    "        self.rate_limit = rate_limit\n",
    "        self.last_request_time = 0\n",
    "        self.user_agent_rotator = UserAgent()\n",
    "    \n",
    "    async def is_within_quota(self) -> bool:\n",
    "        \"\"\"Checks if the API is within its usage quota.\"\"\"\n",
    "        return self.used < self.quota\n",
    "    \n",
    "    async def respect_rate_limit(self):\n",
    "        \"\"\"Pauses execution to respect the API's rate limit.\"\"\"\n",
    "        time_since_last_request = time.time() - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit:\n",
    "            await asyncio.sleep(self.rate_limit - time_since_last_request)\n",
    "    \n",
    "    async def search(self, query: str, num_results: int) -> List[SearchResult]:\n",
    "        \"\"\"Performs a search using the API.\"\"\"\n",
    "        await self.respect_rate_limit()\n",
    "        logger.info(f\"Searching {self.name} for: {query}\")\n",
    "        params = self.params.copy()\n",
    "        params['q'] = query\n",
    "        params['num'] = min(num_results, 10) if self.name == 'Google' else num_results\n",
    "        headers = {'User-Agent': self.user_agent_rotator.random}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            self.used += 1\n",
    "            self.last_request_time = time.time()\n",
    "            data = response.json()\n",
    "            \n",
    "            results = []\n",
    "            for item in data.get(self.results_path, []):\n",
    "                url = item.get('link') or item.get('url')\n",
    "                title = item.get('title') or \"No title\"\n",
    "                snippet = item.get('snippet') or \"No snippet\"\n",
    "                results.append(SearchResult(title, url, snippet))\n",
    "            return results\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error searching {self.name}: {e}\")\n",
    "            return []\n",
    "\n",
    "class DuckDuckGoSearchProvider(SearchProvider):\n",
    "    \"\"\"Provides search functionality using DuckDuckGo.\"\"\"\n",
    "    \n",
    "    async def search(self, query: str, max_results: int) -> List[SearchResult]:\n",
    "        \"\"\"Searches DuckDuckGo and returns a list of SearchResult objects.\"\"\"\n",
    "        try:\n",
    "            with DDGS() as ddgs:\n",
    "                results = []\n",
    "                for r in ddgs.text(query, max_results=max_results):\n",
    "                    result = SearchResult(\n",
    "                        title=r.get('title', ''),\n",
    "                        url=r.get('link', ''),\n",
    "                        snippet=r.get('body', '')\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DuckDuckGo search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _sanitize_query(self, query: str) -> str:\n",
    "        \"\"\"Sanitizes the search query for DuckDuckGo.\"\"\"\n",
    "        return query.strip()\n",
    "\n",
    "def initialize_apis() -> List[SearchAPI]:\n",
    "    \"\"\"Initializes the APIs.\"\"\"\n",
    "    if GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY is None or GOOGLE_CUSTOM_SEARCH_ENGINE_ID is None:\n",
    "        raise ValueError(\"GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY and GOOGLE_CUSTOM_SEARCH_ENGINE_ID must be set in .env.\")\n",
    "    \n",
    "    apis = [\n",
    "        SearchAPI(\n",
    "            \"Google\",\n",
    "            GOOGLE_CUSTOM_SEARCH_ENGINE_API_KEY,\n",
    "            \"https://www.googleapis.com/customsearch/v1\",\n",
    "            {\"cx\": GOOGLE_CUSTOM_SEARCH_ENGINE_ID},\n",
    "            100,\n",
    "            'items',\n",
    "            1,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    if BRAVE_SEARCH_API_KEY:\n",
    "        apis.append(SearchAPI(\"Brave\", BRAVE_SEARCH_API_KEY, \"https://api.search.brave.com/res/v1/web/search\",\n",
    "                            {}, 2000, 'results', 1))\n",
    "    \n",
    "    apis.append(SearchAPI(\"DuckDuckGo\", \"\", \"https://api.duckduckgo.com/\",\n",
    "                         {\"format\": \"json\"}, float('inf'), 'RelatedTopics', 0))\n",
    "    \n",
    "    return apis\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
